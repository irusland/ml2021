{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Энтропия и критерий Джини"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_i$ - вероятность нахождения системы в _i_-ом состоянии. \n",
    "\n",
    "`Энтропия Шеннона` определяется для системы с _N_ возможными состояниями следующим образом \n",
    "$$ S = - \\sum_{i=1}^{N} p_i \\log_2 p_i $$\n",
    "\n",
    "`Критерий Джини (Gini Impurity)`. Максимизацию этого критерия можно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве. \n",
    "\n",
    "В общем случае критерий Джини считается как\n",
    "$$ G = 1 - \\sum_{k} (p_k)^2 $$\n",
    "\n",
    "Необходимо посчитать, значения `Энтропии` и `критерия Джини`\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y = np.array([1,1,1,1,1,1,0,0,0,0])\n",
    "```\n",
    "#### Output\n",
    "```python\n",
    "0.971, 0.480\n",
    "```\n",
    "Результат проверяется с точность до 3ех цифр после запятой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def calc_criteria(y: np.ndarray) -> (float, float):\n",
    "    assert y.ndim == 1\n",
    "    return entropy(y), gini_impurity(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "y = np.array([1,1,1,1,1,1,0,0,0,0])\n",
    "assert_almost_equal(calc_criteria(y), (0.971, 0.480), decimal=3)\n",
    "######################################################\n",
    "assert_almost_equal(calc_criteria(np.zeros(10)), (0, 0))\n",
    "assert_almost_equal(calc_criteria(np.ones(10)), (0, 0))\n",
    "######################################################\n",
    "y = np.array([1,0,1,0,1,0,1,0])\n",
    "assert_almost_equal(calc_criteria(y), (1, 0.5), decimal=2)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам надо реализовать функцию `inform_gain`, которая будет вычислять прирост информации для критерия (энтропия или критерий Джини) при разбиении выборки по признаку (threshold). \n",
    "\n",
    "Прирост информации при разбиении выборки по признаку _Q_ (например $ x \\leq 12$) определяется как.\n",
    "$$ IG(Q) = S_0 - \\sum_{i=1}^{q} \\frac{N_i}{N} S_i $$ \n",
    "где $q$ - число групп после разбиения. $N_i$ - число элементов выборки, у которых признак _Q_ имеет _i_-ое значение.  \n",
    "\n",
    "И написать функцию `get_best_threshold`, которая будет находить наилучшее разбиение выборки.\n",
    "\n",
    "Не забудьте добавить свои функции из предыдущей задачи.\n",
    "\n",
    "На вход подается:\n",
    "\n",
    "* $X$ - одномерный массив - значения признака.\n",
    "* $y$ - значения бинарных классов.\n",
    "* **criteria\\_func** - функция критерия, для которой вычисляется наилучшее разбиение.\n",
    "* **thr** - значение разбиения \n",
    "\n",
    "### Sample 1 (Inform gain)\n",
    "\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "threshold=3,\n",
    "criteria_func=entropy\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "0.61\n",
    "```\n",
    "\n",
    "### Sample 2 (Get best threshold)\n",
    "\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "criteria_func=entropy\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "4, 1\n",
    "```\n",
    "P.S.\n",
    "`theshold` разбивает выборку по правилу `X <= threshold`, `X > threshold`\n",
    "\n",
    "Для примера 2 допустимо значение `threshold` в диапазоне $[4,5)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: np.ndarray) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func=entropy) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func=entropy) -> (float, float):\n",
    "    assert X.ndim == 1\n",
    "    assert y.ndim == 1\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ            \n",
    "    return best_threshold, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "\n",
    "assert_almost_equal(entropy(y), 1)\n",
    "\n",
    "assert_almost_equal(inform_gain(X, y, 4.5, entropy), 1)\n",
    "assert_almost_equal(inform_gain(X, y, 3, entropy), 0.61, decimal=2)\n",
    "\n",
    "thr, score = get_best_threshold(X, y, entropy)\n",
    "assert 4 <= thr < 5\n",
    "assert_almost_equal(score, 1)\n",
    "######################################################\n",
    "assert_almost_equal(inform_gain(X, y, 4.5, gini_impurity), 0.5)\n",
    "assert_almost_equal(inform_gain(X, y, 2, gini_impurity), 0.21, decimal=2)\n",
    "\n",
    "thr, score = get_best_threshold(X, y, gini_impurity)\n",
    "assert 4 <= thr < 5\n",
    "assert_almost_equal(score, 0.5)\n",
    "######################################################\n",
    "assert_almost_equal(inform_gain(np.ones(10), np.zeros(10), 1, entropy), 0)\n",
    "assert_almost_equal(inform_gain(np.ones(10), np.ones(10), 1, entropy), 0)\n",
    "thr, score = get_best_threshold(np.ones(10), np.zeros(10), entropy)\n",
    "assert_almost_equal(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Best Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию `find_best_split`, которая находит наилучшее разбиение по всем признакам. На вход подается обучающая выборка и функция критерий. Необходимо вернуть: индекс фичи, значение границы (threshold) и результат разбиение (information gain).\n",
    "\n",
    "Добавьте сюда все свои функции из предыдущих задач.\n",
    "\n",
    "### Sample 1\n",
    "\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y = np.array([1, 1, 0, 0])\n",
    "criteria_func=entropy\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "0, -1.0, 1.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: np.ndarray) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func=entropy) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n",
    "\n",
    "def get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func=entropy) -> (float, float):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ            \n",
    "    return best_threshold, best_score\n",
    "\n",
    "def find_best_split(X, y, criteria_func=entropy):\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    best_feature = 0\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    \n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ    \n",
    "    \n",
    "    return best_feature, best_threshold, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "\n",
    "ftr, thr, scr = find_best_split(X_clf, y_clf, entropy)\n",
    "assert ftr == 0\n",
    "assert -1 <= thr < 1\n",
    "assert scr == 1\n",
    "######################################################\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "random_state = 42\n",
    "X, y = make_classification(n_samples=30, n_features=4, n_informative=2,\n",
    "                           scale=5, random_state=random_state)\n",
    "X = X.astype(np.int)\n",
    "ftr, _, scr = find_best_split(X, y, entropy)\n",
    "assert ftr == 2\n",
    "assert_almost_equal(scr, 0.71, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Мое дерево решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача реализовать свой простой `DecisionTreeClassifier` для бинарных данных. Вам нужно реализовать 3 метода:\n",
    "\n",
    "* `fit` - обучение классификатора\n",
    "* `predict` - предсказание для новых объектов\n",
    "* `predict_proba` - предсказание вероятностей новых объектов\n",
    "\n",
    "У нашего классификатора будет лишь два гиперпараметра - максимальная глубина дерева `max_depth` и критерий разбиения `criterion`. Энтропия или Джини.\n",
    "\n",
    "**Все** функции из предыдущих заданий нужно добавить в этот код.\n",
    "\n",
    "На вход будет подаваться выборка объектов $X$. $y$ - результат бинарной классификации $0$ или $1$.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = MyDecisionTreeClassifier(max_depth=2, criterion='entropy').fit(X_clf, y_clf)\n",
    "y_pred = model.predict(np.array([[2, 2], [-2, -2]]))\n",
    "y_prob = model.predict_proba(np.array([[2, 2], [-2, -2]]))\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([1, 0])\n",
    "y_prob = np.array([[0.0, 1.0], [1.0, 0.0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MyDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=4, criterion='entropy'): \n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion # 'entropy' or 'gini' \n",
    "        self.tree = {}\n",
    "        self._criteria_func = {\n",
    "            'gini': _gini_impurity,\n",
    "            'entropy': _entropy\n",
    "        }\n",
    "    def _entropy(y: np.ndarray) -> float:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "\n",
    "    def _gini_impurity(y: np.ndarray) -> float:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "        \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return {}\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        return self        \n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X: np.ndarray): # получаем \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = MyDecisionTreeClassifier(max_depth=2).fit(X_clf, y_clf)\n",
    "\n",
    "assert_equal(model.predict(np.array([[2, 2], [-2, -2]])), np.array([1, 0]))\n",
    "\n",
    "assert_almost_equal(model.predict_proba(np.array([[2, 2], [-2, -2]])),\n",
    "                    np.array([[0, 1.0], [1.0, 0.0]]))\n",
    "######################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "random_state = 42\n",
    "X, y = make_classification(n_samples=500, n_features=4, n_informative=2,\n",
    "                           scale=5, random_state=random_state)\n",
    "X = X.astype(np.int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=random_state, stratify=y)\n",
    "######################################################\n",
    "my_dtc = MyDecisionTreeClassifier(max_depth=4, criterion='entropy').fit(X_train, y_train)\n",
    "my_acc = accuracy_score(y_test, my_dtc.predict(X_test))\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=4, criterion='entropy').fit(X_train, y_train)\n",
    "dtc_acc = accuracy_score(y_test, dtc.predict(X_test))\n",
    "assert_almost_equal(my_acc, dtc_acc, decimal=2)\n",
    "######################################################\n",
    "my_dtc = MyDecisionTreeClassifier(max_depth=4, criterion='gini').fit(X_train, y_train)\n",
    "my_acc = accuracy_score(y_test, my_dtc.predict(X_test))\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=4, criterion='gini').fit(X_train, y_train)\n",
    "dtc_acc = accuracy_score(y_test, dtc.predict(X_test))\n",
    "assert_almost_equal(my_acc, dtc_acc, decimal=2)\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "На вход массив чисел $X$ и число бутстрепных выборок $B$. Необходимо реализовать свой бутстреп и найти матожидание и стандартную ошибку у бутстрепных выборок.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 100000\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "42.1, 4.56\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem # ищет SE среднего\n",
    "\n",
    "def get_stats(X: np.array, B:int)->tuple:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    return mean, SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 100000\n",
    "\n",
    "mean, se = get_stats(X, B)\n",
    "\n",
    "assert np.abs(mean - 42.1) < 0.05\n",
    "assert np.abs(se - 4.56) < 0.03\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bias-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается **один** объект $(x, y)$ и список из нескольких **обученных** моделей. \n",
    "\n",
    "Необходимо найти $error$, $bias^2$, $variance$ для данного объекта.\n",
    "\n",
    "Теперь все аккуратно запишем, чтобы не запутаться.\n",
    "\n",
    "* $(x, y)$ - тестировачная выборка\n",
    "* $a_1(\\cdot), \\ldots, a_M(\\cdot)$ - модели (это не обученные на бутстрепе модели, а просто возможные модели из пространства $\\mathbb{A}$, которое мы выбрали)\n",
    "\n",
    "Как настоящие статистики мы можем ~~забить~~ оценить матожидание как среднее. **Это не смешанная модель, а именно оценка матожидания через среднее**\n",
    "$$\\mathbb{E}a(x) = \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)$$\n",
    "\n",
    "**Error** (берем матожидание от квадрата разности)\n",
    "\n",
    "$$error = \\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{M}\\sum_{i=1}^{M}(a_i(x) - y)^2$$\n",
    "\n",
    "**Bias** (заметьте, что возвращаем квадрат bias, а не просто bias)\n",
    "\n",
    "$$bias^2 = \\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\Big(y - \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)\\Big)^2$$  \n",
    "\n",
    "\n",
    "**Variance** (ищем смещенную оценку)\n",
    "\n",
    "$$variance = \\mathbb{D}_{a}a(x)= \\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(a_i(x)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x)\\Big)^2$$\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "x, y = np.array([[0,0,0]]), 0\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=1),  #already fitted estimators\n",
    "              DecisionTreeRegressor(max_depth=5, random_state=1)]\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "error, bias2, var = 3.574, 3.255, 0.319\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bias_variance_decomp(x_test:np.array, y_test:int, estimators:list)->tuple:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "\n",
    "    return error, bias2, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n_samples, noise, f):\n",
    "    X = np.linspace(-4, 4, n_samples)\n",
    "    y = f(X)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "######################################################\n",
    "\n",
    "n_train = 150        \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "X, y = generate(n_samples=n_train, noise=noise, f=f)\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=2, random_state=1).fit(X, y), \n",
    "              DecisionTreeRegressor(max_depth=4, random_state=1).fit(X, y)]\n",
    "\n",
    "x, y = np.array([[2]]), 1.5\n",
    "\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([0.108, 0.083, 0.025]), decimal=3)\n",
    "\n",
    "x, y = np.array([[-0.7]]), 0.8\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([0.045, 0.002, 0.043]), decimal=3)\n",
    "\n",
    "######################################################\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10, \n",
    "                       n_targets=1, shuffle=False, random_state=10)\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=1).fit(X, y), \n",
    "              DecisionTreeRegressor(max_depth=5, random_state=1).fit(X, y)]\n",
    "\n",
    "x, y = np.array([[0,0,0]]), 0\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.574, 3.255, 0.319]), decimal=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bias-variance v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь тоже самое, только для нескольких объектов\n",
    "\n",
    "На вход подается тестовая выборка объект $(X_{test}, y_{test})$ и список из нескольких **обученных** моделей. \n",
    "\n",
    "Необходимо найти $error$, $bias^2$, $variance$, $noise$ для данного объекта.\n",
    "\n",
    "$$error = \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}(a_j(x_i) - y_i)^2$$\n",
    "\n",
    "$$bias^2 = \\mathbb{E}_{x,y}\\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(y_i - \\frac{1}{M}\\sum_{j=1}^{M}a_j(x_i)\\Big)^2$$  \n",
    "\n",
    "$$variance = \\mathbb{E}_{x,y}\\mathbb{D}_{a}a(x)= \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}\\Big(a_j(x_i)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x_i)\\Big)^2$$\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "x = np.array([[  0,   0,   0],\n",
    "              [0.1, 0.1, 0.1]])\n",
    "y = np.array([0, 0.1])\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=3), \n",
    "              DecisionTreeRegressor(max_depth=5, random_state=3)]\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "error, bias2, var = 3.399, 3.079, 0.319\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bias_variance_decomp2(x_test:np.array, y_test:np.array, estimators:list)->tuple:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "\n",
    "    return error, bias2, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n_samples, noise, f):\n",
    "    X = np.linspace(-4, 4, n_samples)\n",
    "    y = f(X)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "######################################################\n",
    "\n",
    "n_train = 150        \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "X, y = generate(n_samples=n_train, noise=noise, f=f)\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=2, random_state=1).fit(X, y), \n",
    "              DecisionTreeRegressor(max_depth=4, random_state=1).fit(X, y)]\n",
    "\n",
    "x = np.array([[2], [-0.7]]) \n",
    "y = np.array([1.5, 0.8])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          (np.array([0.108, 0.083, 0.025]) + np.array([0.045, 0.002, 0.043])) / 2, decimal=3)\n",
    "\n",
    "######################################################\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10, \n",
    "                       n_targets=1, shuffle=False, random_state=10)\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=1).fit(X, y), \n",
    "              DecisionTreeRegressor(max_depth=5, random_state=1).fit(X, y)]\n",
    "\n",
    "x = np.array([[  0,   0,   0]])\n",
    "y = np.array([0])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.574, 3.255, 0.319]), decimal=3)\n",
    "\n",
    "\n",
    "x = np.array([[  0,   0,   0],\n",
    "              [0.1, 0.1, 0.1]])\n",
    "y = np.array([0, 0.1])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.399, 3.079, 0.319]), decimal=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается лист **необученных** алгоритмов регрессии, тренировочная и тестовые выборки. \n",
    "\n",
    "Необходимо \n",
    "\n",
    "* бустингом сделать несколько выборок $X_1, \\ldots, X_B$\n",
    "* обучить алгоритмы на этих выборках: $a_1(\\cdot), \\ldots, a_B(\\cdot)$\n",
    "* реализовать бэггинг алгоритмов и найти собственно предсказания, $error$, $bias^2$ и $variance$.\n",
    "\n",
    "Вот теперь аккуратно. Это - **не матожидание**! Это модель такая.\n",
    "$$a(x) = \\frac{1}{B}\\sum_{b=1}^{B}a_b(x)$$\n",
    "\n",
    "А вот ее матожидание равно для всех алгоритмов:\n",
    "$$\\mathbb{E}_aa(x) = \\mathbb{E}_a\\frac{1}{B}\\sum_{b=1}^{B}a_b(x) = \\mathbb{E}_aa_1(x)$$\n",
    "\n",
    "Но так как теперь, нам нужно посчитать матожидание, мы воспользуемся нашим множеством алгоритмов, обученных на бутстрепе, чтобы получить оценку матожидания единичного алгоритма.\n",
    "\n",
    "$$\\mathbb{E}_aa_1(x) = \\frac{1}{B}\\sum_{j=1}^{B}a_j(x)$$\n",
    "\n",
    "Остальные формулы берутся из предыдущей задачи.\n",
    "\n",
    "P.S.\n",
    "\n",
    "* Так как тут есть вероятности, в целом тесты могут `редко` не взлететь. Перезашлите задачу в этом случае.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "boot_count = 1000\n",
    "estimators = [DecisionTreeRegressor(max_depth=2) for _ in range(boot_count)]\n",
    "X_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\n",
    "y_train = np.array([0, 1, 5, 8, 10])\n",
    "X_test  = np.array([[4, 4], [6, 6]])\n",
    "y_test  = np.array([4, 6])\n",
    "\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3.656 6.039])\n",
    "error  = 3.7 \n",
    "bias^2 = 0.1\n",
    "var    = 3.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bagging(estimator, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "\n",
    "    return y_pred, loss, bias, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "######################################################\n",
    "boot_count=1000\n",
    "estimators = [DecisionTreeRegressor(max_depth=2) for _ in range(boot_count)]\n",
    "X_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\n",
    "y_train = np.array([0, 1, 5, 8, 10])\n",
    "X_test  = np.array([[4, 4], [6, 6]])\n",
    "y_test  = np.array([4, 6])\n",
    "\n",
    "\n",
    "y_pred, loss, bias, var = bagging(estimators, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Да я в курсе что очень грубые ограничения, просто пример игрушечный на таком малом количестве данных\n",
    "assert_array_almost_equal(y_pred, np.array([4, 6]), decimal=0)\n",
    "\n",
    "assert_almost_equal(loss, 3.7, decimal=0) \n",
    "assert_almost_equal(bias, 0.1, decimal=1) \n",
    "assert_almost_equal(var,  3.7, decimal=0) \n",
    "\n",
    "######################################################\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "boot_count=1000\n",
    "estimators = [DecisionTreeRegressor(max_depth=7) for _ in range(boot_count)]\n",
    "\n",
    "\n",
    "y_pred, loss, bias, var = bagging(\n",
    "        estimators, X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert_almost_equal(loss, 32, decimal=0) \n",
    "assert_almost_equal(bias, 14, decimal=0) \n",
    "assert_almost_equal(var,  18, decimal=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) RF Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось переделать чуток предыдущую задачу в `RandomForest`. \n",
    "Но теперь мы наконец попробуем классификацию. (Пока только бинарную)\n",
    "\n",
    "План\n",
    "\n",
    "* Также делаем бутстрепные выборки (для каждого дерева своя выборка, соответственно кол-во выборок - `n_estimators`, в каждой элементов как в начальной выборке `X_train`)\n",
    "* Бэггинг теперь будет только по деревьям классификации\n",
    "* Будем передавать параметр `n_estimators`, `max_depth` и `max_features`\n",
    "\n",
    "Как выбирать ответ в задаче классификации?\n",
    "\n",
    "* Для каждого внутреннего дерева решений находим веротности обоих классов для каждого объекта $X_test$:\n",
    "  * Вызываем `predict_proba` у `DecisionTreeClassifier`\n",
    "* Усредняем вероятности класса и объекта по деревьям:\n",
    "  * $P(n_{class}=d, object=x_k) = \\frac{1}{B}\\sum_{i=1}^{B}P(n_{class}=d, object=x_k, tree=b_i)$\n",
    "* Для каждого объекта выбираем тот класс, у которого выше вероятность\n",
    "\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6]])\n",
    "y_test  = np.array([0, 1])\n",
    "\n",
    "B = 1000\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRFC():\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None):\n",
    "        self.estimators_ = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X_test)-> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6]])\n",
    "y_test  = np.array([0, 1])\n",
    "\n",
    "B = 1000\n",
    "\n",
    "y_pred_my = MyRFC(n_estimators = 100, max_depth=3).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert_array_almost_equal(y_pred_my, np.array([0, 1]))\n",
    "######################################################\n",
    "from random import gauss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_samples = 1000\n",
    "theta = np.linspace(0, 2*np.pi, num_samples)\n",
    "\n",
    "r1 = 1\n",
    "r2 = 2\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "circle = np.hstack([np.cos(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8), \n",
    "                    np.sin(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8)])\n",
    "lil = r1 * circle\n",
    "big = r2 * circle\n",
    "X = np.vstack([lil, big])\n",
    "y = np.hstack([np.zeros(num_samples), np.ones(num_samples)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "y_pred_my = MyRFC(n_estimators = 100, \n",
    "                  max_depth=1).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert accuracy_score(y_pred_my, y_test) > 0.85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто верните отсортированный массив важности фич, полученные из обученного RandomForest. Фичи нумеруются с 1.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
    "y = np.array([0,0,1,1])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "features= np.array([1, 2])\n",
    "importance = np.array([0.75, 0.25])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X, y):\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    return features, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
    "y = np.array([0,0,1,1])\n",
    "\n",
    "f, i = feature_importance(X, y)\n",
    "\n",
    "assert_array_equal(f , np.array([1, 2]))\n",
    "assert i[0] > 0.74\n",
    "######################################################\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=4,\n",
    "                           n_informative=2,\n",
    "                           shuffle=False, \n",
    "                           random_state=10)\n",
    "\n",
    "n = 10\n",
    "a = np.zeros((n, X.shape[1]))\n",
    "for i in range(n):\n",
    "    a[i], _ = feature_importance(X, y) \n",
    "\n",
    "assert_array_equal(np.round(a.mean(axis=0)), np.array([2,3,4,1]))\n",
    "\n",
    "######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
