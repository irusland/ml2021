{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "from numpy import dot\n",
    "from numpy import diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Матричные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются вектор $\\boldsymbol{x}$ длины $n$, $\\boldsymbol{c}$ длины $m$ и матрица $A$ размера $\\{n\\times m\\}$\n",
    "\n",
    "Пусть $y = \\boldsymbol{x}^TA\\boldsymbol{c}$\n",
    "\n",
    "Найдите $\\frac{dy}{d\\boldsymbol{x}}$, $\\frac{dy}{dA}$ и $\\frac{dy}{d\\boldsymbol{c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hint\n",
    "tr(ABC) = tr(CAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hint\n",
    "Транспонирование одномерного массива возвращает одномерный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "def matrix_deriv(x: np.array, A: np.array, c: np.array):\n",
    "    return dot(A, c), dot(c, x.T).T, dot(x.T, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSDYNrQmWK6T"
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "x = np.array([ [1], [2], [3],  [4]])\n",
    "c = np.array([[25], [5],[-9]])\n",
    "A = np.arange(12).reshape(4,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y1, y2, y3 = matrix_deriv(x, A, c)\n",
    "\n",
    "assert_array_equal(y1, np.array([[-13],\n",
    "                                [ 50],\n",
    "                                [113],\n",
    "                                [176]]))\n",
    "\n",
    "assert_array_equal(y2, np.array([[ 25,   5,  -9],\n",
    "                                [ 50,  10, -18],\n",
    "                                [ 75,  15, -27],\n",
    "                                [100,  20, -36]]))\n",
    "                   \n",
    "assert_array_equal(y3, np.array([[60, 70, 80]]))\n",
    "######################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Честная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "В случае линейной регрессии по функционалу $MSE$ задачу оптимизации можно записать следующим образом:\n",
    "\n",
    "$$ \\frac{1}{n}\\sum_{k=1}^N (\\boldsymbol{x}_k^T\\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n",
    "\n",
    "Мы уже знаем, что решение будет выглядеть следующим образом:\n",
    "\n",
    "$$\\boldsymbol{w} = \\begin{cases} X^{-1}\\boldsymbol{y}, & n = m\\\\\n",
    "(X^TX)^{-1}X^T\\boldsymbol{y}, & n > m\\\\\n",
    "X^{T}(XX^{T})^{-1}\\boldsymbol{y}, & n < m\n",
    "\\end{cases}$$\n",
    "\n",
    "где $n$ - количество объектов, а $m$ - количество признаков + 1 (дополнительно 1 вес без признака).\n",
    "\n",
    "Решите задачу регрессии честно, используя формулу выше. \n",
    "\n",
    "Чтобы вы не запутались мы сразу написали добавление единичного столбца к матрице $X$.\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1]])\n",
    "y_train = np.array([2])\n",
    "X_test = np.array([[0.], [2.], [3.]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([1., 3., 4.])\n",
    "model.w_ == np.array([[1.], [1.]])\n",
    "model.coef_ == np.array([1., 1.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.linalg import inv as pinv\n",
    "\n",
    "class TrueLinReg():\n",
    "    def __init__(self):\n",
    "        self.w_ = None  # столбец\n",
    "        self.coef_ = None # строка\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) #добавляем вес без признака\n",
    "        \n",
    "        n, m = X.shape\n",
    "        if n == m:\n",
    "            self.w_ = dot(pinv(X), y)\n",
    "        elif n > m:\n",
    "            self.w_ = dot(dot(pinv(dot(X.T, X)), X.T), y)\n",
    "        else:\n",
    "            self.w_ = dot(dot(X.T, pinv(dot(X, X.T))), y)\n",
    "        \n",
    "        self.w_ = self.w_[np.newaxis].T\n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "#         print(n, m)\n",
    "#         print(self.w_[np.newaxis].T)\n",
    "#         print(self.coef_)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        return (X @ self.w_).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [3], [4]])), np.array([0, 3, 4]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2], [3]])\n",
    "y_reg = np.array([1, -2, 1])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0],[4]])), np.array([0., 0.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1]])\n",
    "y_reg = np.array([2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0.], [2.], [3.]])), np.array([1., 3., 4.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [1.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 1.]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=9, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=3)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=3)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=15, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Честная регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте регуляризацию с коэффициентом $\\lambda$ и решите задачу регрессии (для любого соотношения $m$ и $n$ используйте формулу)\n",
    "$$\\boldsymbol{w} = (X^TX + \\lambda E)^{-1}X^T\\boldsymbol{y}$$\n",
    "где $E$ - единичная матрица.\n",
    "\n",
    "Не забудьте сами добавить **справа** единичный столбец к матрице $X$ аналогично предыдущей задаче.\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "lamb = 1\n",
    "\n",
    "X_test = np.array([[0], [4]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0.33, 3])\n",
    "model.w_ == np.array([[0.67], [0.33]])\n",
    "model.coef_ == np.array([0.67, 0.33])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "class TrueReg():\n",
    "    def __init__(self, lamb):\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) #добавляем вес без признака\n",
    "        \n",
    "        XTX = dot(X.T, X)\n",
    "        \n",
    "        El = self.lamb*np.eye(XTX.shape[0])\n",
    "        XpE = XTX + El\n",
    "        invXE = inv(XpE)\n",
    "        invXEXT = dot(invXE, X.T)\n",
    "        invXEXTy = dot(invXEXT, y)\n",
    "        print(X)\n",
    "        print(XTX)\n",
    "        print(El)\n",
    "        print(XpE)\n",
    "        print(invXE)\n",
    "        print(invXEXT)\n",
    "        print(invXEXTy)\n",
    "        self.w_ = invXEXTy\n",
    "        self.w_ = self.w_[np.newaxis].T\n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "\n",
    "        print(self.w_)\n",
    "        print(self.coef_)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        return (X @ self.w_).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [2. 1.]]\n",
      "[[5. 3.]\n",
      " [3. 2.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[[6. 3.]\n",
      " [3. 3.]]\n",
      "[[ 0.33333333 -0.33333333]\n",
      " [-0.33333333  0.66666667]]\n",
      "[[0.         0.33333333]\n",
      " [0.33333333 0.        ]]\n",
      "[0.66666667 0.33333333]\n",
      "[[0.66666667]\n",
      " [0.33333333]]\n",
      "[0.66666667 0.33333333]\n",
      "[[-1.52040716  0.53069119  1.07434373 ... -0.43876062  0.54900418\n",
      "   1.        ]\n",
      " [ 2.00827588  0.97574774 -0.31654045 ...  0.20716195  0.60608045\n",
      "   1.        ]\n",
      " [ 0.17806492 -1.98507464 -0.52362565 ...  0.73587884  0.9305471\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.69592762 -0.55814767  0.13504361 ... -1.13030292 -1.81830701\n",
      "   1.        ]\n",
      " [ 0.436121   -0.00985649 -0.44218453 ... -0.50480725  0.97702327\n",
      "   1.        ]\n",
      " [ 0.44108316  0.69646602  0.53025543 ...  0.37326543 -0.67255341\n",
      "   1.        ]]\n",
      "[[ 99.39561491 -16.29224054  -6.4309917  ...   2.2611232   -3.40058478\n",
      "    1.91161783]\n",
      " [-16.29224054 101.46829117  -9.67898224 ... -19.98426244   0.56030653\n",
      "  -14.23125556]\n",
      " [ -6.4309917   -9.67898224 102.47744912 ... -10.41879379   5.41850617\n",
      "   -2.1859102 ]\n",
      " ...\n",
      " [  2.2611232  -19.98426244 -10.41879379 ...  83.98463817   1.09424908\n",
      "    0.92749568]\n",
      " [ -3.40058478   0.56030653   5.41850617 ...   1.09424908 103.78903219\n",
      "   -3.14177163]\n",
      " [  1.91161783 -14.23125556  -2.1859102  ...   0.92749568  -3.14177163\n",
      "  100.        ]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "[[100.39561491 -16.29224054  -6.4309917  ...   2.2611232   -3.40058478\n",
      "    1.91161783]\n",
      " [-16.29224054 102.46829117  -9.67898224 ... -19.98426244   0.56030653\n",
      "  -14.23125556]\n",
      " [ -6.4309917   -9.67898224 103.47744912 ... -10.41879379   5.41850617\n",
      "   -2.1859102 ]\n",
      " ...\n",
      " [  2.2611232  -19.98426244 -10.41879379 ...  84.98463817   1.09424908\n",
      "    0.92749568]\n",
      " [ -3.40058478   0.56030653   5.41850617 ...   1.09424908 104.78903219\n",
      "   -3.14177163]\n",
      " [  1.91161783 -14.23125556  -2.1859102  ...   0.92749568  -3.14177163\n",
      "  101.        ]]\n",
      "[[ 0.09872734 -0.00628467  0.00707264 ... -0.00336518 -0.01116477\n",
      "   0.00626687]\n",
      " [-0.00628467  0.08385759  0.01228742 ...  0.01993214 -0.00546641\n",
      "   0.03675891]\n",
      " [ 0.00707264  0.01228742  0.09202964 ... -0.00369737  0.01336808\n",
      "  -0.00259925]\n",
      " ...\n",
      " [-0.00336518  0.01993214 -0.00369737 ...  0.08598717 -0.01695298\n",
      "   0.01363583]\n",
      " [-0.01116477 -0.00546641  0.01336808 ... -0.01695298  0.08669914\n",
      "  -0.00945985]\n",
      " [ 0.00626687  0.03675891 -0.00259925 ...  0.01363583 -0.00945985\n",
      "   0.144108  ]]\n",
      "[[-0.0108114   0.03642498  0.00290436 ... -0.03841928 -0.03299593\n",
      "   0.01673515]\n",
      " [ 0.00433873  0.01380881 -0.0366411  ...  0.00616332 -0.01738653\n",
      "   0.01925015]\n",
      " [-0.00664723 -0.01395429  0.00654025 ... -0.0337807  -0.00300322\n",
      "  -0.01530927]\n",
      " ...\n",
      " [-0.0374238  -0.01675238  0.00759933 ... -0.00088053 -0.02087826\n",
      "   0.01142346]\n",
      " [ 0.01613135 -0.00878045  0.00668395 ... -0.01822516 -0.03459011\n",
      "  -0.01366702]\n",
      " [ 0.03381934  0.01323333  0.02219221 ... -0.01140366  0.02762274\n",
      "   0.03322804]]\n",
      "[-0.75169835 -4.88816411 -3.43216386  0.65263127  0.9338756  -5.15964385\n",
      "  3.56044444 -2.39104691  4.03065737 -2.98916905  0.40082507  5.71395531\n",
      " -0.6566001   0.84238534  6.30299743 -5.29876919 62.14018749 65.2133201\n",
      " -1.50346074  4.04306675  3.80520614 -1.59510714 53.51242559  0.70108092\n",
      "  0.31167613 -1.07772542 -6.07180432 -0.41727002  0.52064712  0.66669488\n",
      "  1.05074379  5.81391633 61.944339    4.2320311   0.73745382  1.95737286\n",
      " -0.79314873  2.87151341  5.88768412 -0.38620593 -2.17118025 -2.67303319\n",
      " -6.09918173 -4.42961989 -0.19056601  2.39634049 -1.36392659 -1.89788914\n",
      " 67.80815518 -3.13163824  4.12458505  1.99304867 -2.55085652 -2.8353875\n",
      "  4.74957418  1.84526121 -1.12186201 -1.88283209 79.97649213 34.70176207\n",
      "  0.26314373  1.96789691  1.34858178  1.54621972  0.74680732 -0.25975694\n",
      "  1.96834013  3.08641609 -0.87384043 -3.02011931 -5.08844759  3.34570489\n",
      " -2.5889723  45.27088276  4.58063683  0.62864627  2.58152227  1.1268072\n",
      "  1.31268611 -1.19719744 -5.74672199 -1.99846243 -3.92335796  4.80598807\n",
      " -3.56296337 -4.2680461   4.73704623 -1.07472564  6.17440798  3.08798003\n",
      " -2.01827865  3.01988688 -3.27648445 -2.7195616   0.86313403  4.94565948\n",
      " 56.15779798  0.39290174 61.66415201 -8.57669317]\n",
      "[[-0.75169835]\n",
      " [-4.88816411]\n",
      " [-3.43216386]\n",
      " [ 0.65263127]\n",
      " [ 0.9338756 ]\n",
      " [-5.15964385]\n",
      " [ 3.56044444]\n",
      " [-2.39104691]\n",
      " [ 4.03065737]\n",
      " [-2.98916905]\n",
      " [ 0.40082507]\n",
      " [ 5.71395531]\n",
      " [-0.6566001 ]\n",
      " [ 0.84238534]\n",
      " [ 6.30299743]\n",
      " [-5.29876919]\n",
      " [62.14018749]\n",
      " [65.2133201 ]\n",
      " [-1.50346074]\n",
      " [ 4.04306675]\n",
      " [ 3.80520614]\n",
      " [-1.59510714]\n",
      " [53.51242559]\n",
      " [ 0.70108092]\n",
      " [ 0.31167613]\n",
      " [-1.07772542]\n",
      " [-6.07180432]\n",
      " [-0.41727002]\n",
      " [ 0.52064712]\n",
      " [ 0.66669488]\n",
      " [ 1.05074379]\n",
      " [ 5.81391633]\n",
      " [61.944339  ]\n",
      " [ 4.2320311 ]\n",
      " [ 0.73745382]\n",
      " [ 1.95737286]\n",
      " [-0.79314873]\n",
      " [ 2.87151341]\n",
      " [ 5.88768412]\n",
      " [-0.38620593]\n",
      " [-2.17118025]\n",
      " [-2.67303319]\n",
      " [-6.09918173]\n",
      " [-4.42961989]\n",
      " [-0.19056601]\n",
      " [ 2.39634049]\n",
      " [-1.36392659]\n",
      " [-1.89788914]\n",
      " [67.80815518]\n",
      " [-3.13163824]\n",
      " [ 4.12458505]\n",
      " [ 1.99304867]\n",
      " [-2.55085652]\n",
      " [-2.8353875 ]\n",
      " [ 4.74957418]\n",
      " [ 1.84526121]\n",
      " [-1.12186201]\n",
      " [-1.88283209]\n",
      " [79.97649213]\n",
      " [34.70176207]\n",
      " [ 0.26314373]\n",
      " [ 1.96789691]\n",
      " [ 1.34858178]\n",
      " [ 1.54621972]\n",
      " [ 0.74680732]\n",
      " [-0.25975694]\n",
      " [ 1.96834013]\n",
      " [ 3.08641609]\n",
      " [-0.87384043]\n",
      " [-3.02011931]\n",
      " [-5.08844759]\n",
      " [ 3.34570489]\n",
      " [-2.5889723 ]\n",
      " [45.27088276]\n",
      " [ 4.58063683]\n",
      " [ 0.62864627]\n",
      " [ 2.58152227]\n",
      " [ 1.1268072 ]\n",
      " [ 1.31268611]\n",
      " [-1.19719744]\n",
      " [-5.74672199]\n",
      " [-1.99846243]\n",
      " [-3.92335796]\n",
      " [ 4.80598807]\n",
      " [-3.56296337]\n",
      " [-4.2680461 ]\n",
      " [ 4.73704623]\n",
      " [-1.07472564]\n",
      " [ 6.17440798]\n",
      " [ 3.08798003]\n",
      " [-2.01827865]\n",
      " [ 3.01988688]\n",
      " [-3.27648445]\n",
      " [-2.7195616 ]\n",
      " [ 0.86313403]\n",
      " [ 4.94565948]\n",
      " [56.15779798]\n",
      " [ 0.39290174]\n",
      " [61.66415201]\n",
      " [-8.57669317]]\n",
      "[-0.75169835 -4.88816411 -3.43216386  0.65263127  0.9338756  -5.15964385\n",
      "  3.56044444 -2.39104691  4.03065737 -2.98916905  0.40082507  5.71395531\n",
      " -0.6566001   0.84238534  6.30299743 -5.29876919 62.14018749 65.2133201\n",
      " -1.50346074  4.04306675  3.80520614 -1.59510714 53.51242559  0.70108092\n",
      "  0.31167613 -1.07772542 -6.07180432 -0.41727002  0.52064712  0.66669488\n",
      "  1.05074379  5.81391633 61.944339    4.2320311   0.73745382  1.95737286\n",
      " -0.79314873  2.87151341  5.88768412 -0.38620593 -2.17118025 -2.67303319\n",
      " -6.09918173 -4.42961989 -0.19056601  2.39634049 -1.36392659 -1.89788914\n",
      " 67.80815518 -3.13163824  4.12458505  1.99304867 -2.55085652 -2.8353875\n",
      "  4.74957418  1.84526121 -1.12186201 -1.88283209 79.97649213 34.70176207\n",
      "  0.26314373  1.96789691  1.34858178  1.54621972  0.74680732 -0.25975694\n",
      "  1.96834013  3.08641609 -0.87384043 -3.02011931 -5.08844759  3.34570489\n",
      " -2.5889723  45.27088276  4.58063683  0.62864627  2.58152227  1.1268072\n",
      "  1.31268611 -1.19719744 -5.74672199 -1.99846243 -3.92335796  4.80598807\n",
      " -3.56296337 -4.2680461   4.73704623 -1.07472564  6.17440798  3.08798003\n",
      " -2.01827865  3.01988688 -3.27648445 -2.7195616   0.86313403  4.94565948\n",
      " 56.15779798  0.39290174 61.66415201 -8.57669317]\n",
      "[[ 0.17651303 -0.87187372  0.64429999 ... -0.84801517 -0.33284552\n",
      "   1.        ]\n",
      " [ 0.24986098 -0.05470875  1.32259085 ...  0.00527443 -0.46239924\n",
      "   1.        ]\n",
      " [-0.17823475 -0.33203814 -0.18405061 ...  0.29705346 -0.5493713\n",
      "   1.        ]\n",
      " ...\n",
      " [ 1.09698145  1.02381303  0.01479836 ...  0.56757899  0.55010217\n",
      "   1.        ]\n",
      " [-0.10302438  1.04518773 -0.303667   ...  0.42044424  2.5064105\n",
      "   1.        ]\n",
      " [-0.27253848 -2.07900556  1.48826182 ... -0.48563288 -0.88050358\n",
      "   1.        ]]\n",
      "[[1972.69496661   19.69822669   16.74676904   41.5921297    94.97319452\n",
      "    -4.00534064  -24.35762349    6.29003656    9.08047034   -9.27461612\n",
      "    86.5645705 ]\n",
      " [  19.69822669 2016.99606367  -30.80503403   -6.79149606    5.06126007\n",
      "   -17.09249914  -15.79725962   23.19415784  -45.62099176  -25.09827247\n",
      "   -59.99738174]\n",
      " [  16.74676904  -30.80503403 1922.16721105   24.75260513    9.15836167\n",
      "    35.1069193   -31.34553946   26.11959863   40.57295125   16.44273606\n",
      "   -12.27020327]\n",
      " [  41.5921297    -6.79149606   24.75260513 1960.11983512   59.77957858\n",
      "   -24.7460017    40.31245904 -100.77193787   57.42189207   60.38365821\n",
      "    34.77659072]\n",
      " [  94.97319452    5.06126007    9.15836167   59.77957858 2094.5352177\n",
      "   -58.60437806  -18.26617967  -25.70675561  -69.66101711   -8.39440843\n",
      "    11.97144995]\n",
      " [  -4.00534064  -17.09249914   35.1069193   -24.7460017   -58.60437806\n",
      "  2059.0456487    -6.4353539     2.48427256   60.38277232   35.42820655\n",
      "    40.18187574]\n",
      " [ -24.35762349  -15.79725962  -31.34553946   40.31245904  -18.26617967\n",
      "    -6.4353539  2149.15964422   14.21675287   38.32380311   18.58770984\n",
      "   -19.71425363]\n",
      " [   6.29003656   23.19415784   26.11959863 -100.77193787  -25.70675561\n",
      "     2.48427256   14.21675287 2015.42180879   10.5340837   -30.50935861\n",
      "   -29.20037125]\n",
      " [   9.08047034  -45.62099176   40.57295125   57.42189207  -69.66101711\n",
      "    60.38277232   38.32380311   10.5340837  2057.81342049   85.38382389\n",
      "   102.60917171]\n",
      " [  -9.27461612  -25.09827247   16.44273606   60.38365821   -8.39440843\n",
      "    35.42820655   18.58770984  -30.50935861   85.38382389 2039.49201332\n",
      "    44.55457421]\n",
      " [  86.5645705   -59.99738174  -12.27020327   34.77659072   11.97144995\n",
      "    40.18187574  -19.71425363  -29.20037125  102.60917171   44.55457421\n",
      "  2000.        ]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[1973.69496661   19.69822669   16.74676904   41.5921297    94.97319452\n",
      "    -4.00534064  -24.35762349    6.29003656    9.08047034   -9.27461612\n",
      "    86.5645705 ]\n",
      " [  19.69822669 2017.99606367  -30.80503403   -6.79149606    5.06126007\n",
      "   -17.09249914  -15.79725962   23.19415784  -45.62099176  -25.09827247\n",
      "   -59.99738174]\n",
      " [  16.74676904  -30.80503403 1923.16721105   24.75260513    9.15836167\n",
      "    35.1069193   -31.34553946   26.11959863   40.57295125   16.44273606\n",
      "   -12.27020327]\n",
      " [  41.5921297    -6.79149606   24.75260513 1961.11983512   59.77957858\n",
      "   -24.7460017    40.31245904 -100.77193787   57.42189207   60.38365821\n",
      "    34.77659072]\n",
      " [  94.97319452    5.06126007    9.15836167   59.77957858 2095.5352177\n",
      "   -58.60437806  -18.26617967  -25.70675561  -69.66101711   -8.39440843\n",
      "    11.97144995]\n",
      " [  -4.00534064  -17.09249914   35.1069193   -24.7460017   -58.60437806\n",
      "  2060.0456487    -6.4353539     2.48427256   60.38277232   35.42820655\n",
      "    40.18187574]\n",
      " [ -24.35762349  -15.79725962  -31.34553946   40.31245904  -18.26617967\n",
      "    -6.4353539  2150.15964422   14.21675287   38.32380311   18.58770984\n",
      "   -19.71425363]\n",
      " [   6.29003656   23.19415784   26.11959863 -100.77193787  -25.70675561\n",
      "     2.48427256   14.21675287 2016.42180879   10.5340837   -30.50935861\n",
      "   -29.20037125]\n",
      " [   9.08047034  -45.62099176   40.57295125   57.42189207  -69.66101711\n",
      "    60.38277232   38.32380311   10.5340837  2058.81342049   85.38382389\n",
      "   102.60917171]\n",
      " [  -9.27461612  -25.09827247   16.44273606   60.38365821   -8.39440843\n",
      "    35.42820655   18.58770984  -30.50935861   85.38382389 2040.49201332\n",
      "    44.55457421]\n",
      " [  86.5645705   -59.99738174  -12.27020327   34.77659072   11.97144995\n",
      "    40.18187574  -19.71425363  -29.20037125  102.60917171   44.55457421\n",
      "  2001.        ]]\n",
      "[[ 5.09115079e-04 -5.59075960e-06 -4.29996854e-06 -9.95391615e-06\n",
      "  -2.26484321e-05  7.05364384e-07  5.48502745e-06 -2.55617032e-06\n",
      "  -1.91467901e-06  2.93867405e-06 -2.18748332e-05]\n",
      " [-5.59075960e-06  4.96554761e-04  7.91300703e-06  7.41567374e-07\n",
      "  -6.82793750e-07  3.32638264e-06  3.64167710e-06 -5.56288156e-06\n",
      "   9.75992697e-06  5.09585168e-06  1.44440876e-05]\n",
      " [-4.29996854e-06  7.91300703e-06  5.20892550e-04 -6.69813333e-06\n",
      "  -2.54005052e-06 -8.67343993e-06  7.97770410e-06 -7.17611470e-06\n",
      "  -9.89983426e-06 -3.64498519e-06  4.48588073e-06]\n",
      " [-9.95391615e-06  7.41567374e-07 -6.69813333e-06  5.13201620e-04\n",
      "  -1.42249542e-05  6.58889391e-06 -9.79421545e-06  2.53894924e-05\n",
      "  -1.37953314e-05 -1.41374922e-05 -7.25846474e-06]\n",
      " [-2.26484321e-05 -6.82793750e-07 -2.54005052e-06 -1.42249542e-05\n",
      "   4.79677630e-04  1.30212066e-05  3.71733011e-06  5.37168573e-06\n",
      "   1.63554192e-05  1.49831518e-06 -2.69733734e-06]\n",
      " [ 7.05364384e-07  3.32638264e-06 -8.67343993e-06  6.58889391e-06\n",
      "   1.30212066e-05  4.86750386e-04  1.56724030e-06 -2.24530648e-07\n",
      "  -1.30306192e-05 -7.75236334e-06 -9.09778651e-06]\n",
      " [ 5.48502745e-06  3.64167710e-06  7.97770410e-06 -9.79421545e-06\n",
      "   3.71733011e-06  1.56724030e-06  4.65762851e-04 -3.82919289e-06\n",
      "  -8.49531145e-06 -3.77238374e-06  5.08987006e-06]\n",
      " [-2.55617032e-06 -5.56288156e-06 -7.17611470e-06  2.53894924e-05\n",
      "   5.37168573e-06 -2.24530648e-07 -3.82919289e-06  4.97674934e-04\n",
      "  -3.57806841e-06  6.73219790e-06  6.68926230e-06]\n",
      " [-1.91467901e-06  9.75992697e-06 -9.89983426e-06 -1.37953314e-05\n",
      "   1.63554192e-05 -1.30306192e-05 -8.49531145e-06 -3.57806841e-06\n",
      "   4.89624267e-04 -1.90451634e-05 -2.41009292e-05]\n",
      " [ 2.93867405e-06  5.09585168e-06 -3.64498519e-06 -1.41374922e-05\n",
      "   1.49831518e-06 -7.75236334e-06 -3.77238374e-06  6.73219790e-06\n",
      "  -1.90451634e-05  4.91882231e-04 -9.51890690e-06]\n",
      " [-2.18748332e-05  1.44440876e-05  4.48588073e-06 -7.25846474e-06\n",
      "  -2.69733734e-06 -9.09778651e-06  5.08987006e-06  6.68926230e-06\n",
      "  -2.41009292e-05 -9.51890690e-06  5.03077600e-04]]\n",
      "[[ 8.82073308e-05  7.03981974e-05 -1.33647964e-04 ...  5.28234950e-04\n",
      "  -1.29233953e-04 -1.52494122e-04]\n",
      " [-4.14333401e-04 -1.17353478e-05 -1.44965509e-04 ...  5.17153784e-04\n",
      "   5.32053739e-04 -1.02178095e-03]\n",
      " [ 3.54161307e-04  6.75914334e-04 -1.07003702e-04 ...  1.06333160e-06\n",
      "  -1.94147730e-04  7.82553244e-04]\n",
      " ...\n",
      " [-4.59761326e-04 -3.28546238e-05  1.26852847e-04 ...  2.57676202e-04\n",
      "   1.70600052e-04 -2.20660769e-04]\n",
      " [-1.80099121e-04 -2.45501725e-04 -3.05342562e-04 ...  2.72492788e-04\n",
      "   1.23157488e-03 -4.02566921e-04]\n",
      " [ 5.02964648e-04  5.05265686e-04  4.77068842e-04 ...  4.83740804e-04\n",
      "   4.78194277e-04  5.24852928e-04]]\n",
      "[3.24172238e+01 5.14435080e+00 9.23482176e+01 4.32748135e+01\n",
      " 5.09268223e+01 9.51639802e+01 3.43051910e+01 2.65101919e+01\n",
      " 3.37596858e+01 8.26132301e+01 2.78707405e-03]\n",
      "[[3.24172238e+01]\n",
      " [5.14435080e+00]\n",
      " [9.23482176e+01]\n",
      " [4.32748135e+01]\n",
      " [5.09268223e+01]\n",
      " [9.51639802e+01]\n",
      " [3.43051910e+01]\n",
      " [2.65101919e+01]\n",
      " [3.37596858e+01]\n",
      " [8.26132301e+01]\n",
      " [2.78707405e-03]]\n",
      "[3.24172238e+01 5.14435080e+00 9.23482176e+01 4.32748135e+01\n",
      " 5.09268223e+01 9.51639802e+01 3.43051910e+01 2.65101919e+01\n",
      " 3.37596858e+01 8.26132301e+01 2.78707405e-03]\n",
      "[[ 0.83398522  0.83562286  1.19566678 ... -0.26063562 -0.5472008\n",
      "   1.        ]\n",
      " [-0.83656743 -0.32329831  0.7006622  ...  0.26688448  0.6445013\n",
      "   1.        ]\n",
      " [-0.61501943  0.43796819  0.0467888  ... -0.49835967  0.42628126\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.10915924  0.50847417 -0.25220129 ... -1.01187081 -0.46050776\n",
      "   1.        ]\n",
      " [ 1.39150829 -0.41099132  1.22932212 ...  0.54264037 -0.57695035\n",
      "   1.        ]\n",
      " [-0.9580462   1.86342778  0.0679925  ...  0.39332567 -0.62051654\n",
      "   1.        ]]\n",
      "[[ 8.99006437e+01  4.26652215e+00 -1.02509115e+00 ... -3.94045587e-02\n",
      "  -8.64468950e+00  1.37744015e+01]\n",
      " [ 4.26652215e+00  9.68448232e+01  1.11565925e+01 ...  3.48264547e+00\n",
      "   1.77035471e+00  6.47918410e+00]\n",
      " [-1.02509115e+00  1.11565925e+01  9.80653339e+01 ...  1.42304542e+01\n",
      "   7.13819187e+00  2.82604382e+00]\n",
      " ...\n",
      " [-3.94045587e-02  3.48264547e+00  1.42304542e+01 ...  1.00405899e+02\n",
      "  -2.73604230e+00  1.16507354e+01]\n",
      " [-8.64468950e+00  1.77035471e+00  7.13819187e+00 ... -2.73604230e+00\n",
      "   7.63553977e+01  2.76079140e+00]\n",
      " [ 1.37744015e+01  6.47918410e+00  2.82604382e+00 ...  1.16507354e+01\n",
      "   2.76079140e+00  1.00000000e+02]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "[[ 9.09006437e+01  4.26652215e+00 -1.02509115e+00 ... -3.94045587e-02\n",
      "  -8.64468950e+00  1.37744015e+01]\n",
      " [ 4.26652215e+00  9.78448232e+01  1.11565925e+01 ...  3.48264547e+00\n",
      "   1.77035471e+00  6.47918410e+00]\n",
      " [-1.02509115e+00  1.11565925e+01  9.90653339e+01 ...  1.42304542e+01\n",
      "   7.13819187e+00  2.82604382e+00]\n",
      " ...\n",
      " [-3.94045587e-02  3.48264547e+00  1.42304542e+01 ...  1.01405899e+02\n",
      "  -2.73604230e+00  1.16507354e+01]\n",
      " [-8.64468950e+00  1.77035471e+00  7.13819187e+00 ... -2.73604230e+00\n",
      "   7.73553977e+01  2.76079140e+00]\n",
      " [ 1.37744015e+01  6.47918410e+00  2.82604382e+00 ...  1.16507354e+01\n",
      "   2.76079140e+00  1.01000000e+02]]\n",
      "[[ 0.34489079  0.04411552  0.04614801 ... -0.01444211 -0.0461125\n",
      "  -0.07242028]\n",
      " [ 0.04411552  0.34557991 -0.04787271 ... -0.01864887  0.01835307\n",
      "  -0.03897211]\n",
      " [ 0.04614801 -0.04787271  0.23961664 ... -0.06240335  0.03468549\n",
      "   0.02638498]\n",
      " ...\n",
      " [-0.01444211 -0.01864887 -0.06240335 ...  0.28348839 -0.00133961\n",
      "  -0.01511094]\n",
      " [-0.0461125   0.01835307  0.03468549 ... -0.00133961  0.39541405\n",
      "   0.00769997]\n",
      " [-0.07242028 -0.03897211  0.02638498 ... -0.01511094  0.00769997\n",
      "   0.31444101]]\n",
      "[[-0.00121843 -0.00552938 -0.01003427 ...  0.00149554  0.0116964\n",
      "  -0.00189303]\n",
      " [ 0.01021707  0.01097779  0.00016465 ...  0.0096264   0.01210204\n",
      "  -0.00180832]\n",
      " [ 0.00233106  0.00712825 -0.00430407 ...  0.00385426  0.01781257\n",
      "   0.00533962]\n",
      " ...\n",
      " [-0.01170133 -0.00406053 -0.00265815 ... -0.0007454  -0.00171048\n",
      "  -0.00024616]\n",
      " [-0.0148975   0.00122109  0.00041982 ...  0.00077658 -0.01207375\n",
      "  -0.00718837]\n",
      " [ 0.01694359  0.02091385  0.01334364 ...  0.00046311  0.01218612\n",
      "   0.00791844]]\n",
      "[-1.46198928e+01  3.91774835e+00  3.85613642e-01 -1.53161737e+00\n",
      " -2.99468999e+00 -8.69517564e+00  1.80529937e+00 -3.37541666e+00\n",
      "  7.72833438e+00  1.01042123e+01  6.21455661e+00  7.72969890e+00\n",
      "  3.13758929e+00 -1.35749445e+00 -6.33760127e+00 -1.71808422e+00\n",
      "  4.23174717e+00  7.75192540e+00  1.20641868e+01  4.03238621e+00\n",
      "  2.84665380e+00 -1.22996127e+00  4.43504886e+00 -2.73840495e+00\n",
      " -6.58692981e-01 -4.77682865e-01  2.77518428e+00 -6.47681006e+00\n",
      " -4.27022807e+00  3.31849069e+01  2.22290812e+00 -3.10462126e+00\n",
      "  6.87809365e+00 -2.23140144e+00  2.25078290e+00 -4.60317529e+00\n",
      " -2.65100911e+00  6.70867004e+00 -1.02790526e+00 -6.35808852e+00\n",
      "  2.89598547e+00 -4.13209899e+00 -4.79184651e+00 -1.06403626e+01\n",
      " -6.71283937e-02 -2.93743605e+00 -3.84092301e-02  4.87373022e+01\n",
      " -2.75495286e+00 -4.44939253e+00  5.88637369e+00 -5.55726258e-01\n",
      "  4.32673642e+00  1.45232929e+01 -6.90553740e-01 -8.35164227e+00\n",
      " -3.75568387e+00  3.01521145e+00  2.58600619e+00 -1.09171303e+01\n",
      " -6.25219498e+00  9.27047309e+00 -5.26035766e+00 -4.62186673e+00\n",
      " -2.75561397e-01 -6.25323210e+00  7.69413539e-01 -8.85307135e-01\n",
      " -1.88233444e+00  3.36877626e+00  2.21467539e+00 -8.72917121e-01\n",
      "  1.05288552e+01  1.11162847e+00 -1.49024171e+00  7.44131952e+00\n",
      "  4.79621548e+00  1.33764988e+01 -8.83078785e-01  9.19311571e+00\n",
      "  6.17424955e+00 -1.33943291e+00  8.25041344e+00  9.03226465e+00\n",
      "  7.81612207e+00  1.99861402e+01  1.89083598e+00 -6.06337562e+00\n",
      "  2.29568251e+01 -5.37076086e+00 -7.54316399e+00 -4.73624528e+00\n",
      "  5.76558561e+00  2.41651125e+01 -2.63701431e+00  5.33846157e+00\n",
      "  1.66648377e+00  4.81198714e-01  1.41254552e+00 -1.52603005e+00\n",
      "  3.24946524e+00  9.16211441e-01 -4.65401658e+00  1.25672588e+01\n",
      "  4.76170577e+00 -1.34785220e+00 -3.57210252e+00  6.75047030e+00\n",
      " -3.41450751e+00  6.99031472e+00 -8.90525987e+00  1.81785194e-01\n",
      "  3.41972109e+00  4.40211416e+00 -2.09452253e+00  6.12587322e+00\n",
      " -3.30819396e+00  1.08798550e+01 -4.77608012e+00  3.87545667e+00\n",
      "  3.49373481e+00  3.25484645e+01  7.10838739e+00 -1.30553897e+00\n",
      " -1.03411217e-01 -6.39471445e+00  1.04054720e+01 -1.71798924e+00\n",
      "  1.18976965e+00 -1.99591594e+00  4.98743094e+00  8.56028705e+00\n",
      " -4.85781848e+00  8.56324985e-01 -6.03118037e-01 -4.57350904e+00\n",
      "  2.92182165e-01 -7.93219224e+00  4.99647990e+00  1.59384834e+00\n",
      "  1.02611885e+01  7.51401759e+00  3.85189419e+00  7.36701141e+00\n",
      "  4.29042994e+00  2.73958826e+00  3.98778179e+01 -5.10094795e+00\n",
      " -4.70401884e+00  2.25291710e+00  5.28304163e+00]\n",
      "[[-1.46198928e+01]\n",
      " [ 3.91774835e+00]\n",
      " [ 3.85613642e-01]\n",
      " [-1.53161737e+00]\n",
      " [-2.99468999e+00]\n",
      " [-8.69517564e+00]\n",
      " [ 1.80529937e+00]\n",
      " [-3.37541666e+00]\n",
      " [ 7.72833438e+00]\n",
      " [ 1.01042123e+01]\n",
      " [ 6.21455661e+00]\n",
      " [ 7.72969890e+00]\n",
      " [ 3.13758929e+00]\n",
      " [-1.35749445e+00]\n",
      " [-6.33760127e+00]\n",
      " [-1.71808422e+00]\n",
      " [ 4.23174717e+00]\n",
      " [ 7.75192540e+00]\n",
      " [ 1.20641868e+01]\n",
      " [ 4.03238621e+00]\n",
      " [ 2.84665380e+00]\n",
      " [-1.22996127e+00]\n",
      " [ 4.43504886e+00]\n",
      " [-2.73840495e+00]\n",
      " [-6.58692981e-01]\n",
      " [-4.77682865e-01]\n",
      " [ 2.77518428e+00]\n",
      " [-6.47681006e+00]\n",
      " [-4.27022807e+00]\n",
      " [ 3.31849069e+01]\n",
      " [ 2.22290812e+00]\n",
      " [-3.10462126e+00]\n",
      " [ 6.87809365e+00]\n",
      " [-2.23140144e+00]\n",
      " [ 2.25078290e+00]\n",
      " [-4.60317529e+00]\n",
      " [-2.65100911e+00]\n",
      " [ 6.70867004e+00]\n",
      " [-1.02790526e+00]\n",
      " [-6.35808852e+00]\n",
      " [ 2.89598547e+00]\n",
      " [-4.13209899e+00]\n",
      " [-4.79184651e+00]\n",
      " [-1.06403626e+01]\n",
      " [-6.71283937e-02]\n",
      " [-2.93743605e+00]\n",
      " [-3.84092301e-02]\n",
      " [ 4.87373022e+01]\n",
      " [-2.75495286e+00]\n",
      " [-4.44939253e+00]\n",
      " [ 5.88637369e+00]\n",
      " [-5.55726258e-01]\n",
      " [ 4.32673642e+00]\n",
      " [ 1.45232929e+01]\n",
      " [-6.90553740e-01]\n",
      " [-8.35164227e+00]\n",
      " [-3.75568387e+00]\n",
      " [ 3.01521145e+00]\n",
      " [ 2.58600619e+00]\n",
      " [-1.09171303e+01]\n",
      " [-6.25219498e+00]\n",
      " [ 9.27047309e+00]\n",
      " [-5.26035766e+00]\n",
      " [-4.62186673e+00]\n",
      " [-2.75561397e-01]\n",
      " [-6.25323210e+00]\n",
      " [ 7.69413539e-01]\n",
      " [-8.85307135e-01]\n",
      " [-1.88233444e+00]\n",
      " [ 3.36877626e+00]\n",
      " [ 2.21467539e+00]\n",
      " [-8.72917121e-01]\n",
      " [ 1.05288552e+01]\n",
      " [ 1.11162847e+00]\n",
      " [-1.49024171e+00]\n",
      " [ 7.44131952e+00]\n",
      " [ 4.79621548e+00]\n",
      " [ 1.33764988e+01]\n",
      " [-8.83078785e-01]\n",
      " [ 9.19311571e+00]\n",
      " [ 6.17424955e+00]\n",
      " [-1.33943291e+00]\n",
      " [ 8.25041344e+00]\n",
      " [ 9.03226465e+00]\n",
      " [ 7.81612207e+00]\n",
      " [ 1.99861402e+01]\n",
      " [ 1.89083598e+00]\n",
      " [-6.06337562e+00]\n",
      " [ 2.29568251e+01]\n",
      " [-5.37076086e+00]\n",
      " [-7.54316399e+00]\n",
      " [-4.73624528e+00]\n",
      " [ 5.76558561e+00]\n",
      " [ 2.41651125e+01]\n",
      " [-2.63701431e+00]\n",
      " [ 5.33846157e+00]\n",
      " [ 1.66648377e+00]\n",
      " [ 4.81198714e-01]\n",
      " [ 1.41254552e+00]\n",
      " [-1.52603005e+00]\n",
      " [ 3.24946524e+00]\n",
      " [ 9.16211441e-01]\n",
      " [-4.65401658e+00]\n",
      " [ 1.25672588e+01]\n",
      " [ 4.76170577e+00]\n",
      " [-1.34785220e+00]\n",
      " [-3.57210252e+00]\n",
      " [ 6.75047030e+00]\n",
      " [-3.41450751e+00]\n",
      " [ 6.99031472e+00]\n",
      " [-8.90525987e+00]\n",
      " [ 1.81785194e-01]\n",
      " [ 3.41972109e+00]\n",
      " [ 4.40211416e+00]\n",
      " [-2.09452253e+00]\n",
      " [ 6.12587322e+00]\n",
      " [-3.30819396e+00]\n",
      " [ 1.08798550e+01]\n",
      " [-4.77608012e+00]\n",
      " [ 3.87545667e+00]\n",
      " [ 3.49373481e+00]\n",
      " [ 3.25484645e+01]\n",
      " [ 7.10838739e+00]\n",
      " [-1.30553897e+00]\n",
      " [-1.03411217e-01]\n",
      " [-6.39471445e+00]\n",
      " [ 1.04054720e+01]\n",
      " [-1.71798924e+00]\n",
      " [ 1.18976965e+00]\n",
      " [-1.99591594e+00]\n",
      " [ 4.98743094e+00]\n",
      " [ 8.56028705e+00]\n",
      " [-4.85781848e+00]\n",
      " [ 8.56324985e-01]\n",
      " [-6.03118037e-01]\n",
      " [-4.57350904e+00]\n",
      " [ 2.92182165e-01]\n",
      " [-7.93219224e+00]\n",
      " [ 4.99647990e+00]\n",
      " [ 1.59384834e+00]\n",
      " [ 1.02611885e+01]\n",
      " [ 7.51401759e+00]\n",
      " [ 3.85189419e+00]\n",
      " [ 7.36701141e+00]\n",
      " [ 4.29042994e+00]\n",
      " [ 2.73958826e+00]\n",
      " [ 3.98778179e+01]\n",
      " [-5.10094795e+00]\n",
      " [-4.70401884e+00]\n",
      " [ 2.25291710e+00]\n",
      " [ 5.28304163e+00]]\n",
      "[-1.46198928e+01  3.91774835e+00  3.85613642e-01 -1.53161737e+00\n",
      " -2.99468999e+00 -8.69517564e+00  1.80529937e+00 -3.37541666e+00\n",
      "  7.72833438e+00  1.01042123e+01  6.21455661e+00  7.72969890e+00\n",
      "  3.13758929e+00 -1.35749445e+00 -6.33760127e+00 -1.71808422e+00\n",
      "  4.23174717e+00  7.75192540e+00  1.20641868e+01  4.03238621e+00\n",
      "  2.84665380e+00 -1.22996127e+00  4.43504886e+00 -2.73840495e+00\n",
      " -6.58692981e-01 -4.77682865e-01  2.77518428e+00 -6.47681006e+00\n",
      " -4.27022807e+00  3.31849069e+01  2.22290812e+00 -3.10462126e+00\n",
      "  6.87809365e+00 -2.23140144e+00  2.25078290e+00 -4.60317529e+00\n",
      " -2.65100911e+00  6.70867004e+00 -1.02790526e+00 -6.35808852e+00\n",
      "  2.89598547e+00 -4.13209899e+00 -4.79184651e+00 -1.06403626e+01\n",
      " -6.71283937e-02 -2.93743605e+00 -3.84092301e-02  4.87373022e+01\n",
      " -2.75495286e+00 -4.44939253e+00  5.88637369e+00 -5.55726258e-01\n",
      "  4.32673642e+00  1.45232929e+01 -6.90553740e-01 -8.35164227e+00\n",
      " -3.75568387e+00  3.01521145e+00  2.58600619e+00 -1.09171303e+01\n",
      " -6.25219498e+00  9.27047309e+00 -5.26035766e+00 -4.62186673e+00\n",
      " -2.75561397e-01 -6.25323210e+00  7.69413539e-01 -8.85307135e-01\n",
      " -1.88233444e+00  3.36877626e+00  2.21467539e+00 -8.72917121e-01\n",
      "  1.05288552e+01  1.11162847e+00 -1.49024171e+00  7.44131952e+00\n",
      "  4.79621548e+00  1.33764988e+01 -8.83078785e-01  9.19311571e+00\n",
      "  6.17424955e+00 -1.33943291e+00  8.25041344e+00  9.03226465e+00\n",
      "  7.81612207e+00  1.99861402e+01  1.89083598e+00 -6.06337562e+00\n",
      "  2.29568251e+01 -5.37076086e+00 -7.54316399e+00 -4.73624528e+00\n",
      "  5.76558561e+00  2.41651125e+01 -2.63701431e+00  5.33846157e+00\n",
      "  1.66648377e+00  4.81198714e-01  1.41254552e+00 -1.52603005e+00\n",
      "  3.24946524e+00  9.16211441e-01 -4.65401658e+00  1.25672588e+01\n",
      "  4.76170577e+00 -1.34785220e+00 -3.57210252e+00  6.75047030e+00\n",
      " -3.41450751e+00  6.99031472e+00 -8.90525987e+00  1.81785194e-01\n",
      "  3.41972109e+00  4.40211416e+00 -2.09452253e+00  6.12587322e+00\n",
      " -3.30819396e+00  1.08798550e+01 -4.77608012e+00  3.87545667e+00\n",
      "  3.49373481e+00  3.25484645e+01  7.10838739e+00 -1.30553897e+00\n",
      " -1.03411217e-01 -6.39471445e+00  1.04054720e+01 -1.71798924e+00\n",
      "  1.18976965e+00 -1.99591594e+00  4.98743094e+00  8.56028705e+00\n",
      " -4.85781848e+00  8.56324985e-01 -6.03118037e-01 -4.57350904e+00\n",
      "  2.92182165e-01 -7.93219224e+00  4.99647990e+00  1.59384834e+00\n",
      "  1.02611885e+01  7.51401759e+00  3.85189419e+00  7.36701141e+00\n",
      "  4.29042994e+00  2.73958826e+00  3.98778179e+01 -5.10094795e+00\n",
      " -4.70401884e+00  2.25291710e+00  5.28304163e+00]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-140-8fa080edab98>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0mcoef_real\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoef_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintercept_\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean_absolute_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoef_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoef_real\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.67], [0.33]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0.67, 0.33]), decimal=2)\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [4]])), np.array([0.33, 3]), decimal=2)\n",
    "\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=99, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=2000, n_features=10, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=150, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsyTwOrXZmOI"
   },
   "source": [
    "# SLIDE (1) Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOHHlozhZmOP"
   },
   "source": [
    "На вход подаются обучающая выборка $(X,\\boldsymbol{y})$ и как-то определенные веса $w$. Верните градиент функции $MSE$ для изменения весов в алгоритме градиентного спуска.\n",
    "$$\\nabla_{\\boldsymbol{w}} L = \\frac{2}{n} X^{T}(X\\boldsymbol{w} - \\boldsymbol{y})$$\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[0.2], [0.2]]) #столбец!\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[-8.4], \n",
    "       [-8.4]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLzhM5-LZmOX"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XegQEY5ZmOc"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    return dot(X.T, (dot(X, w) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YLSZH2_ZmO5"
   },
   "outputs": [],
   "source": [
    "w = np.array([[0.2], \n",
    "              [0.2]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]])\n",
    "assert_array_almost_equal( gradient_step(w, X, y), np.array([[-5.6],[-5.6]]))\n",
    "######################################################\n",
    "w = np.array([[-1.90059439], [-0.39772511], [-0.41752917], [ 1.52698708],\n",
    "              [ 1.93384661], [ 0.55038402], [-0.43914043], [-0.53450491],\n",
    "              [-0.20436079], [ 1.47930748], [-0.43431298], [-0.67715663],\n",
    "              [ 0.6140469 ], [ 0.33724021], [-0.47389417], [-0.27931867],\n",
    "              [-0.19815547], [-1.10817221], [-0.2783202 ], [-0.26673129]])\n",
    "\n",
    "y = np.array([[-1.35285676], [-1.90639143], [-1.1872863 ], [-0.47531246],\n",
    "              [-0.5658066 ], [-2.01777789], [-0.55111576], [-1.17508504],\n",
    "              [-0.31591632], [-1.10876769]])\n",
    "\n",
    "X = np.array([[-1.79418438, -0.28247459,  0.63485396,  0.18973693, -1.13052678,\n",
    "                -0.70693165,  0.02485169, -1.32269407, -1.19972889, -0.90835517,\n",
    "                1.70735351, -0.31278511,  0.26195237,  1.15186451, -1.06028399,\n",
    "                0.35805288,  0.1097917 , -1.64881113, -0.26764338,  0.52419335],\n",
    "              [-0.39820007, -1.07706627,  0.03495314, -0.36069043, -0.38655026,\n",
    "                0.99804715,  0.62738777,  1.53889357,  0.47893302, -1.17044503,\n",
    "                -1.14344307,  0.99980883, -0.89283077, -0.30342722, -1.64897791,\n",
    "                -1.13511464, -0.82071543,  1.8879236 , -1.5129215 ,  0.12872281],\n",
    "              [-0.26032624, -0.59442306, -0.20607555,  1.0785192 , -0.72817743,\n",
    "                1.60260617, -2.34932775,  0.55693193,  0.25573628,  1.53147877,\n",
    "                1.53834672,  0.1878024 ,  1.18042765,  0.54514643,  0.01803304,\n",
    "                1.51429761, -0.17469223,  0.81986536, -0.46578708,  0.42436921],\n",
    "              [ 0.10641183, -0.18608143,  2.69988291,  0.30114113, -0.56763953,\n",
    "                -0.89837188,  1.19577105, -0.86920194, -2.68921283, -0.58902866,\n",
    "                -0.14408206, -1.84817003, -0.9124478 ,  0.55288917,  1.14731704,\n",
    "                -0.4114787 , -0.53848335,  0.63225441,  0.61095749, -0.16311904],\n",
    "              [ 0.42409436,  1.00368214, -1.47558928,  0.01692064,  0.95318017,\n",
    "                -0.04567367, -1.62167744, -0.20345068, -0.1389324 , -0.4342349 ,\n",
    "                0.41877207, -1.37897995, -0.31973986, -1.6112183 , -0.18485078,\n",
    "                -1.16219592, -1.08054026, -0.49210254, -1.52373135,  2.34733614],\n",
    "              [ 0.19339171, -1.25994886, -1.3263278 , -1.56807527, -1.16484596,\n",
    "                1.51848938, -0.40378262,  0.28889234,  0.10067917,  1.89112864,\n",
    "                -1.87355328,  0.19510271,  1.13826496,  0.41762094,  0.61912712,\n",
    "                -1.06733394, -0.22801313, -1.49211563, -0.97819834, -0.08353847],\n",
    "              [ 0.5225887 ,  0.03288573, -0.1318777 ,  0.75014314, -1.36193554,\n",
    "                -0.71094454,  2.09153476,  1.50715026,  0.03406876, -0.71503137,\n",
    "                1.22186058,  0.9721076 , -0.33217577,  0.60094908, -0.19959933,\n",
    "                -0.79264798, -0.80242992, -1.46592485,  0.00955109, -0.31271658],\n",
    "              [ 1.07854603,  1.63387859, -0.68689403, -1.49026312, -0.05352863,\n",
    "                0.74923652, -0.61221446,  0.25499529, -0.87862293,  2.30763454,\n",
    "                -0.45654804, -0.49537167, -1.14185717,  0.09358794,  1.71154247,\n",
    "                0.51549091, -0.26392429,  1.16472574, -0.53397584, -0.79062073],\n",
    "              [-0.56480156, -1.83123029,  0.99887682, -0.60648359,  0.37281722,\n",
    "                0.19015213,  0.08486153,  1.54633375, -0.22768608,  1.4061507 ,\n",
    "                -0.07928461, -1.03523357,  0.44072784, -0.98970029, -0.10722007,\n",
    "                1.54788238, -0.89526339, -1.14606415, -0.65972113,  0.96906237],\n",
    "              [ 1.36608419,  0.09914266, -0.02822534,  0.97303833,  2.14361812,\n",
    "                0.18005144,  0.94756695,  1.97038803, -1.00026315,  0.47603231,\n",
    "                1.14925524, -1.16030033,  1.42723652,  0.00458275,  0.62078139,\n",
    "                -2.47565446, -0.16698078,  0.25653049,  0.8664139 , -0.37928587]])\n",
    "\n",
    "\n",
    "######################################################\n",
    "g = np.array([-1.0993842672390932, -3.0877233608129395, -2.13575686356165, -0.21916813495587567, \n",
    "              1.6843662270764397, 3.3322365301217958, -5.049140901669979, 1.5001184420835205, \n",
    "              -0.4856689267508395, 4.84907077228488, 1.8736602701312628, -2.8537960600078045, \n",
    "              5.534114292930919, -0.6315258126682273, -0.10153263395507897, 0.10430238830850164, \n",
    "              -0.6327318737851857, -3.3518163054239145, -1.9647292933263618, 3.1983699740883607])\n",
    "\n",
    "assert np.allclose(gradient_step(w, X, y).reshape(20), g)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4PvDGpRZmPU"
   },
   "source": [
    "# SLIDE (1) Стохастический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdPhYgwyZmPV"
   },
   "source": [
    "Реализуйте стохаистический градиентный спуск: пересчитайте $\\nabla_{\\boldsymbol{w}}L$ только для одного случайно выбранного элемента из выборки $X$.\n",
    "\n",
    "$$L_{k} = (\\boldsymbol{x}_k^T \\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{w}}L_{k} = \\boldsymbol{x}_k (\\boldsymbol{x}_k^T \\boldsymbol{w}  - y_k)$$\n",
    "где $\\boldsymbol{x}_i$ - вектор объекта выборки, выбранный случайно.  \n",
    "\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице $X$, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1], [1]]) #столбец!\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "y = np.array([[1], [0]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[0.],\n",
    "       [0.],\n",
    "       [0.]]) #столбец!\n",
    "или\n",
    "array([[2.],\n",
    "       [2.],\n",
    "       [2.]]) #столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuVTpgPEZmPW"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxwmdI1VZmPX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "import random\n",
    "\n",
    "def stochastic_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    k = random.randrange(X.shape[0])\n",
    "    x = X[k]\n",
    "    y = y[k]\n",
    "    xTw = dot(x.T, w)\n",
    "    xTw_y = (xTw - y)\n",
    "    \n",
    "    L =x * xTw_y\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goRVoRRpZmPe"
   },
   "outputs": [],
   "source": [
    "w = np.array([[1], [1], [1]])\n",
    "y = np.array([[1], [0]])\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "\n",
    "z , k = 0 , 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 2:\n",
    "        z += 1\n",
    "    else:\n",
    "        k += 1\n",
    "assert z >= 35 and k >= 35, (z, k)\n",
    "######################################################\n",
    "w = np.array([[5], [2], [1], [5]])\n",
    "y = np.array([[0.5], [2], [-3]])\n",
    "X = np.array([[1, 1, 1, 1], [0, 0, 0, 1], [3, 5, 3, 1]])\n",
    "\n",
    "z , k, f = 0 , 0, 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 108:\n",
    "        z += 1\n",
    "    elif g[0] == 0:\n",
    "        k += 1\n",
    "    else:\n",
    "        f += 1\n",
    "assert z >= 25 and k >= 25 and f >= 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd1Jg4PKZmM1"
   },
   "source": [
    "# SLIDE (1) Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1FBAmquZmM7"
   },
   "source": [
    "Теперь реализуем Линейную регрессию на градиентном спуске. Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса инициализированы нулями. \n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L$$\n",
    "\n",
    "Используйте параметры `max_iter`=$1000$, `eta`=$0.1$\n",
    "\n",
    "Все итерации проходить не нужно. Остановитесь в момент, когда норма разницы между старыми и новыми весами станет меньше $1e-9$. \n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg(max_iter=1000, eta=0.1).fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[3],[4]]))\n",
    "\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3., 4.])\n",
    "model.coef_ = np.array([1., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmlBx1vYZmNE"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F1cIF6-ZmNL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "import random\n",
    "class GDLinReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.1):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.coef_ = None #строка\n",
    "        self.w_ = None #столбец\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # ИЗ ПРЕДЫДУЩЕГО ЗАДАНИЯ # DELETE THIS COMMENT !!!!!!\n",
    "    def _gradient_descending(self, w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "        k = random.randrange(X.shape[0])\n",
    "        x = X[k]\n",
    "        y = y[k]\n",
    "        xTw = dot(x.T, w)\n",
    "        xTw_y = (xTw - y)\n",
    "\n",
    "        L =x * xTw_y\n",
    "        return L\n",
    "    ##########################\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros((m, 1)) #столбец\n",
    "  \n",
    "        vector = self.w_\n",
    "        for _ in range(self.max_iter):\n",
    "            g = self._gradient_descending(vector, X, y)\n",
    "            diff = -self.eta * g\n",
    "            if np.all(np.abs(diff) < 1e-6):\n",
    "                break\n",
    "            vector = (vector.T + diff).T\n",
    "        self.w_ = vector    \n",
    "            \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)   \n",
    "        return (X @ self.w_).reshape(-1) #возвращаем всегда строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3., 4.]), decimal=1)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=1)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model2 = GDLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "coef_my = model2.coef_\n",
    "\n",
    "assert mean_absolute_error(coef_my, coef_real) < 5\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krP0hQnFZmPA"
   },
   "source": [
    "# SLIDE (1) Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwUtXwj_ZmPD"
   },
   "source": [
    "Реализуйте L2-регуляризацию (так же известную как Ridge).\n",
    "\n",
    "$$L = \\frac{1}{2}\\lVert X\\boldsymbol{w} - \\boldsymbol{y}\\rVert_2^2 + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2$$\n",
    "\n",
    "Найдите новый $\\nabla_{\\boldsymbol{w}} L$ и верните его.\n",
    "\n",
    "Обратите внимание, что свободный коэффициент весов (самый последний) **не нужно** регуляризовывать.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "np.array([[28.], \n",
    "          [27.]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3W_KzN3ZmPF"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kN24jsqZmPH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "\n",
    "def norm(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    return dot(X.T, (dot(X, w) - y))\n",
    "\n",
    "def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n",
    "    \n",
    "    error = X.dot(w) - y\n",
    "\n",
    "\n",
    "    cost = (1 / 2) * norm(X.dot(w) - y) + (lamb / 2) * norm(w)\n",
    "\n",
    "    gradient = c\n",
    "    print(error, cost, gradient, sep='\\n')\n",
    "    \n",
    "    return w + lamb * cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n",
    "    nw = np.copy(w)\n",
    "    nw[-1] = 0\n",
    "    return X.T @ (X @ w - y) + lamb * nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRDN31FAZmPQ"
   },
   "outputs": [],
   "source": [
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(gradient_step_l2(w, X, y, 1), np.array([[28.], [27.]]))\n",
    "\n",
    "def gradient_descending(w, X, y, l):\n",
    "    # посчитайте градиент функции ошибок по весам. Переменная l=0, не используйте её.\n",
    "    g = X.T @ (X @ w - y)\n",
    "    return g\n",
    "\n",
    "w = np.array([[-1.90059439], [-0.39772511], [-0.41752917], [ 1.52698708],\n",
    "              [ 1.93384661], [ 0.55038402], [-0.43914043], [-0.53450491],\n",
    "              [-0.20436079], [ 1.47930748], [-0.43431298], [-0.67715663],\n",
    "              [ 0.6140469 ], [ 0.33724021], [-0.47389417], [-0.27931867],\n",
    "              [-0.19815547], [-1.10817221], [-0.2783202 ], [-0.26673129]])\n",
    "\n",
    "y = np.array([[-1.35285676], [-1.90639143], [-1.1872863 ], [-0.47531246],\n",
    "              [-0.5658066 ], [-2.01777789], [-0.55111576], [-1.17508504],\n",
    "              [-0.31591632], [-1.10876769]])\n",
    "\n",
    "X = np.array([[-1.79418438, -0.28247459,  0.63485396,  0.18973693, -1.13052678,\n",
    "                -0.70693165,  0.02485169, -1.32269407, -1.19972889, -0.90835517,\n",
    "                1.70735351, -0.31278511,  0.26195237,  1.15186451, -1.06028399,\n",
    "                0.35805288,  0.1097917 , -1.64881113, -0.26764338,  0.52419335],\n",
    "              [-0.39820007, -1.07706627,  0.03495314, -0.36069043, -0.38655026,\n",
    "                0.99804715,  0.62738777,  1.53889357,  0.47893302, -1.17044503,\n",
    "                -1.14344307,  0.99980883, -0.89283077, -0.30342722, -1.64897791,\n",
    "                -1.13511464, -0.82071543,  1.8879236 , -1.5129215 ,  0.12872281],\n",
    "              [-0.26032624, -0.59442306, -0.20607555,  1.0785192 , -0.72817743,\n",
    "                1.60260617, -2.34932775,  0.55693193,  0.25573628,  1.53147877,\n",
    "                1.53834672,  0.1878024 ,  1.18042765,  0.54514643,  0.01803304,\n",
    "                1.51429761, -0.17469223,  0.81986536, -0.46578708,  0.42436921],\n",
    "              [ 0.10641183, -0.18608143,  2.69988291,  0.30114113, -0.56763953,\n",
    "                -0.89837188,  1.19577105, -0.86920194, -2.68921283, -0.58902866,\n",
    "                -0.14408206, -1.84817003, -0.9124478 ,  0.55288917,  1.14731704,\n",
    "                -0.4114787 , -0.53848335,  0.63225441,  0.61095749, -0.16311904],\n",
    "              [ 0.42409436,  1.00368214, -1.47558928,  0.01692064,  0.95318017,\n",
    "                -0.04567367, -1.62167744, -0.20345068, -0.1389324 , -0.4342349 ,\n",
    "                0.41877207, -1.37897995, -0.31973986, -1.6112183 , -0.18485078,\n",
    "                -1.16219592, -1.08054026, -0.49210254, -1.52373135,  2.34733614],\n",
    "              [ 0.19339171, -1.25994886, -1.3263278 , -1.56807527, -1.16484596,\n",
    "                1.51848938, -0.40378262,  0.28889234,  0.10067917,  1.89112864,\n",
    "                -1.87355328,  0.19510271,  1.13826496,  0.41762094,  0.61912712,\n",
    "                -1.06733394, -0.22801313, -1.49211563, -0.97819834, -0.08353847],\n",
    "              [ 0.5225887 ,  0.03288573, -0.1318777 ,  0.75014314, -1.36193554,\n",
    "                -0.71094454,  2.09153476,  1.50715026,  0.03406876, -0.71503137,\n",
    "                1.22186058,  0.9721076 , -0.33217577,  0.60094908, -0.19959933,\n",
    "                -0.79264798, -0.80242992, -1.46592485,  0.00955109, -0.31271658],\n",
    "              [ 1.07854603,  1.63387859, -0.68689403, -1.49026312, -0.05352863,\n",
    "                0.74923652, -0.61221446,  0.25499529, -0.87862293,  2.30763454,\n",
    "                -0.45654804, -0.49537167, -1.14185717,  0.09358794,  1.71154247,\n",
    "                0.51549091, -0.26392429,  1.16472574, -0.53397584, -0.79062073],\n",
    "              [-0.56480156, -1.83123029,  0.99887682, -0.60648359,  0.37281722,\n",
    "                0.19015213,  0.08486153,  1.54633375, -0.22768608,  1.4061507 ,\n",
    "                -0.07928461, -1.03523357,  0.44072784, -0.98970029, -0.10722007,\n",
    "                1.54788238, -0.89526339, -1.14606415, -0.65972113,  0.96906237],\n",
    "              [ 1.36608419,  0.09914266, -0.02822534,  0.97303833,  2.14361812,\n",
    "                0.18005144,  0.94756695,  1.97038803, -1.00026315,  0.47603231,\n",
    "                1.14925524, -1.16030033,  1.42723652,  0.00458275,  0.62078139,\n",
    "                -2.47565446, -0.16698078,  0.25653049,  0.8664139 , -0.37928587]])\n",
    "\n",
    "######################################################\n",
    "nw = np.copy(w)\n",
    "nw[-1] = 0\n",
    "\n",
    "l = 123\n",
    "assert np.allclose(gradient_descending(w, X, y, 0) + l * nw, gradient_step_l2(w, X, y, l))\n",
    "l = 1\n",
    "assert np.allclose(gradient_descending(w, X, y, 0) + l * nw, gradient_step_l2(w, X, y, l))\n",
    "l = 123\n",
    "assert np.allclose(gradient_descending(w, X, y, 0) + l * nw, gradient_step_l2(w, X, y, l))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Логистический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к задаче классификации и Логистической регрессии. Мы уже знаем, что в нашем случае нужно минимизировать метрику $LogLoss$:\n",
    "$$\\ln{L} = -\\sum_{k=1}^{n}y_i\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))}$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$\n",
    "\n",
    "Ваша задача взять **производную** этой функции и вернуть вектор градиентов $\\nabla_{\\boldsymbol{w}}\\ln{L}$. Формулу необходимо вывести в векторном виде, чтобы считалось быстрее.\n",
    "\n",
    "Обратите внимание: \n",
    "\n",
    "* $w$, $y$ - столбцы, а не строки. Ответ тоже столбец.\n",
    "* В функции используется **натуральный** логарифм.\n",
    "* Нужно посчитать полный градиент, а не стохастический.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([0, 0])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[2], \n",
    "       [2]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_almost_equal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "def log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    return X.T @ (sigmoid(X @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0], [0]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(log_gradient_step(w, X, y), np.array([[2], [2]]))\n",
    "######################################################\n",
    "w = np.array([[-1.90059439], [-0.39772511], [-0.41752917], [ 1.52698708],\n",
    "              [ 1.93384661], [ 0.55038402], [-0.43914043], [-0.53450491],\n",
    "              [-0.20436079], [ 1.47930748], [-0.43431298], [-0.67715663],\n",
    "              [ 0.6140469 ], [ 0.33724021], [-0.47389417], [-0.27931867],\n",
    "              [-0.19815547], [-1.10817221], [-0.2783202 ], [-0.26673129]])\n",
    "\n",
    "y = np.array([[-1.35285676], [-1.90639143], [-1.1872863 ], [-0.47531246],\n",
    "              [-0.5658066 ], [-2.01777789], [-0.55111576], [-1.17508504],\n",
    "              [-0.31591632], [-1.10876769]])\n",
    "\n",
    "X = np.array([[-1.79418438, -0.28247459,  0.63485396,  0.18973693, -1.13052678,\n",
    "                -0.70693165,  0.02485169, -1.32269407, -1.19972889, -0.90835517,\n",
    "                1.70735351, -0.31278511,  0.26195237,  1.15186451, -1.06028399,\n",
    "                0.35805288,  0.1097917 , -1.64881113, -0.26764338,  0.52419335],\n",
    "              [-0.39820007, -1.07706627,  0.03495314, -0.36069043, -0.38655026,\n",
    "                0.99804715,  0.62738777,  1.53889357,  0.47893302, -1.17044503,\n",
    "                -1.14344307,  0.99980883, -0.89283077, -0.30342722, -1.64897791,\n",
    "                -1.13511464, -0.82071543,  1.8879236 , -1.5129215 ,  0.12872281],\n",
    "              [-0.26032624, -0.59442306, -0.20607555,  1.0785192 , -0.72817743,\n",
    "                1.60260617, -2.34932775,  0.55693193,  0.25573628,  1.53147877,\n",
    "                1.53834672,  0.1878024 ,  1.18042765,  0.54514643,  0.01803304,\n",
    "                1.51429761, -0.17469223,  0.81986536, -0.46578708,  0.42436921],\n",
    "              [ 0.10641183, -0.18608143,  2.69988291,  0.30114113, -0.56763953,\n",
    "                -0.89837188,  1.19577105, -0.86920194, -2.68921283, -0.58902866,\n",
    "                -0.14408206, -1.84817003, -0.9124478 ,  0.55288917,  1.14731704,\n",
    "                -0.4114787 , -0.53848335,  0.63225441,  0.61095749, -0.16311904],\n",
    "              [ 0.42409436,  1.00368214, -1.47558928,  0.01692064,  0.95318017,\n",
    "                -0.04567367, -1.62167744, -0.20345068, -0.1389324 , -0.4342349 ,\n",
    "                0.41877207, -1.37897995, -0.31973986, -1.6112183 , -0.18485078,\n",
    "                -1.16219592, -1.08054026, -0.49210254, -1.52373135,  2.34733614],\n",
    "              [ 0.19339171, -1.25994886, -1.3263278 , -1.56807527, -1.16484596,\n",
    "                1.51848938, -0.40378262,  0.28889234,  0.10067917,  1.89112864,\n",
    "                -1.87355328,  0.19510271,  1.13826496,  0.41762094,  0.61912712,\n",
    "                -1.06733394, -0.22801313, -1.49211563, -0.97819834, -0.08353847],\n",
    "              [ 0.5225887 ,  0.03288573, -0.1318777 ,  0.75014314, -1.36193554,\n",
    "                -0.71094454,  2.09153476,  1.50715026,  0.03406876, -0.71503137,\n",
    "                1.22186058,  0.9721076 , -0.33217577,  0.60094908, -0.19959933,\n",
    "                -0.79264798, -0.80242992, -1.46592485,  0.00955109, -0.31271658],\n",
    "              [ 1.07854603,  1.63387859, -0.68689403, -1.49026312, -0.05352863,\n",
    "                0.74923652, -0.61221446,  0.25499529, -0.87862293,  2.30763454,\n",
    "                -0.45654804, -0.49537167, -1.14185717,  0.09358794,  1.71154247,\n",
    "                0.51549091, -0.26392429,  1.16472574, -0.53397584, -0.79062073],\n",
    "              [-0.56480156, -1.83123029,  0.99887682, -0.60648359,  0.37281722,\n",
    "                0.19015213,  0.08486153,  1.54633375, -0.22768608,  1.4061507 ,\n",
    "                -0.07928461, -1.03523357,  0.44072784, -0.98970029, -0.10722007,\n",
    "                1.54788238, -0.89526339, -1.14606415, -0.65972113,  0.96906237],\n",
    "              [ 1.36608419,  0.09914266, -0.02822534,  0.97303833,  2.14361812,\n",
    "                0.18005144,  0.94756695,  1.97038803, -1.00026315,  0.47603231,\n",
    "                1.14925524, -1.16030033,  1.42723652,  0.00458275,  0.62078139,\n",
    "                -2.47565446, -0.16698078,  0.25653049,  0.8664139 , -0.37928587]])\n",
    "\n",
    "g = np.array([[-0.43431449],\n",
    "       [-6.4967537 ],\n",
    "       [-3.33915541],\n",
    "       [-2.60685525],\n",
    "       [-3.13767717],\n",
    "       [ 8.9115332 ],\n",
    "       [-4.30338709],\n",
    "       [ 8.52721835],\n",
    "       [-6.04197056],\n",
    "       [ 8.94208373],\n",
    "       [ 2.41288223],\n",
    "       [-4.65486414],\n",
    "       [ 5.81341316],\n",
    "       [ 1.54813258],\n",
    "       [-0.23116471],\n",
    "       [-6.1655314 ],\n",
    "       [-6.56704074],\n",
    "       [-3.53911584],\n",
    "       [-9.10686695],\n",
    "       [ 4.83364865]])\n",
    "\n",
    "assert_array_almost_equal(log_gradient_step(w, X, y), g)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M2uSfHyZmPh"
   },
   "source": [
    "# SLIDE (1) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocEDVotnZmPi"
   },
   "source": [
    "Теперь реализуем Логистическую регрессию на стохастическом градиентном спуске.\n",
    "$$L = -\\ln{Likelihood} = -\\sum_{k=1}^{n}y_k\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2 \\rightarrow \\min$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$, $\\lVert \\boldsymbol{w} \\rVert_2^2 = \\sum_{i=1}^{m} w_i^2$= - квадрат эвклидовой нормы\n",
    "\n",
    "Собственно вероятность принадлежности определенному классу:\n",
    "$$P(y_k=1) = \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})~~~P(y_k=0) = 1 - \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})$$\n",
    "\n",
    "Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса сгенерированны рандомно. \n",
    "\n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L_k$$\n",
    "\n",
    "Будьте внимательны:\n",
    "\n",
    "* На вход алгоритму приходит $\\boldsymbol{y}$ - **строка**, как и в любой sklearn алгоритм.\n",
    "* Не устанавливайте количество итераций больше 1000, так как алгоритм будет долго работать.\n",
    "* Как и в линейной регрессии не забудьте добавить единичный столбец справа.\n",
    "\n",
    "Можете использовать и обычный спуск, главное чтоб по времени зашло.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = SGDLogReg().fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\n",
    "y_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([1., 0.])\n",
    "y_prob = np.array([[0.1, 0.9],  # это не точный ответ, но очень приблизительный, отличие на 0.1 - это нормально\n",
    "                   [0.9, 0.1]]) # для каждого объекта возвращаем его вероятность нуля и единицы\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FR5V2Xf2KgaG"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "def log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    return X.T @ (sigmoid(X @ w) - y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tI7mwEgKgZ3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDLogReg():\n",
    "    def __init__(self, max_iter=10, eta=0.01, lamb = 1):\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.lamb = lamb\n",
    "        self.w_ = None\n",
    "        self.coef_ = None\n",
    "        self.c = None\n",
    "        \n",
    "    def _stochastic_step(self, w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "        return  X.T @ (sigmoid(X.T @ w).T - y)\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros(shape=(m, 1))\n",
    "\n",
    "        self.c = 0\n",
    "        for epoch in range(self.max_iter):\n",
    "            for i in range(len(X)):\n",
    "                gw = X[i] * (y[i] - sigmoid(np.dot(self.w_.T, X[i])))\n",
    "                gc = y[i] - sigmoid(np.dot(self.w_.T, X[i]) + self.c)\n",
    "\n",
    "                self.w_ = (self.w_.T - (self.eta * gw).T).T\n",
    "                self.c = self.c - self.eta * gc\n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        print(self.w_)\n",
    "        # print(self.c)\n",
    "        # print()\n",
    "        # print(X)\n",
    "        # print(y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        predictions = []\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        # print(X)\n",
    "\n",
    "\n",
    "        y_pred = sigmoid(X @ self.w_)\n",
    "        # print(y_pred, y_pred>=0.5)\n",
    "\n",
    "        # print((y_pred>=0.5).astype(int))\n",
    "        return (y_pred<0.5).astype(int).T[0]\n",
    "    \n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.09501654e-01]\n",
      " [ 2.73188624e-04]\n",
      " [-9.80702840e-10]]\n",
      "[[-57.1701676 ]\n",
      " [-65.85983323]\n",
      " [ -6.6172746 ]]\n",
      "[0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-cd7894d26996>:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "<ipython-input-3-cd7894d26996>:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "<ipython-input-3-cd7894d26996>:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-28-76d0b017ef3b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlamb\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0.01\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_real\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mmean_absolute_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_real\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0.1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mmean_absolute_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_real\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0.2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;31m######################################################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from numpy.testing import assert_equal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "######################################################\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "model = SGDLogReg().fit(X_clf, y_clf)\n",
    "\n",
    "assert model.lamb == 1\n",
    "\n",
    "# assert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\n",
    "# assert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\n",
    "######################################################\n",
    "np.random.seed(1337)\n",
    "n = 200\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=2, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.01).fit(X_train, y_train)\n",
    "model_real = LogisticRegression(C=0.01, solver='sag').fit(X_train, y_train)\n",
    "\n",
    "assert model.lamb == 0.01\n",
    "print(model.predict(X_test), model_real.predict(X_test))\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.1\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.2\n",
    "######################################################\n",
    "np.random.seed(1228)\n",
    "n = 1000\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=1, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.5).fit(X_train, y_train)\n",
    "model_real = SGDClassifier(loss='log',alpha=0.5).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "assert model.lamb == 0.5\n",
    "\n",
    "\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.2\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.25\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ \\boldsymbol{g}^{(t)} = \\gamma \\boldsymbol{g}^{(t - 1)} + \\eta \\nabla_{\\boldsymbol{w}}L_{k}$$\n",
    "$$ \\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\boldsymbol{g}^{(t)}$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\eta$ — learning rate\n",
    " - $\\boldsymbol{w}$ — вектор параметров\n",
    " - $\\boldsymbol{g}$ — вектор градиентов \n",
    " - $L$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDMomentum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n",
    "        self.gradient = gradient #функция находящая градиент за вас\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            cur_grad = self.gradient(self.w, X[index, :], np.array(y[index]), self.l)\n",
    "            # пересчитайте веса в стохаистическом градиентном спуске\n",
    "            v = self.gamma * v + self.lr * cur_grad\n",
    "            self.w = self.w - v\n",
    "        # self.w = self.w /2\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n",
    "         1.63136986e-01, -5.44506820e-01]])\n",
    "y = np.array([-1])\n",
    "\n",
    "r0 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=1)\n",
    "r0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r0.fit(X, y)\n",
    "r1 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=0.1)\n",
    "r1.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r1.fit(X, y)\n",
    "######################################################\n",
    "assert np.allclose(r0.w.reshape(6), np.array([ 0.01753165,  0.0350633,   0.05259494,  0.0350633,   0.01753165, -0.01753165])), r0.w.reshape(6)\n",
    "assert np.allclose(r1.w.reshape(6), np.array([-0.05887894, -0.11775788, -0.17663682, -0.11775788, -0.05887894,  0.05887894]))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}