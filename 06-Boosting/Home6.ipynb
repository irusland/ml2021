{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) X-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо найти наилучшие параметры для XGBRegression, обучить модель и вернуть ее. Данные из гита `Financial Distress.csv`.\n",
    "\n",
    "Сам гридсерч или нативное исследование необходимо делать вне функции обработки, чтобы не получить TL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xreg(X_train, y_train):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:00:46] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Financial Distress.csv')\n",
    "\n",
    "X = df.drop('Financial Distress', axis=1)\n",
    "y = df['Financial Distress']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "\n",
    "xgb_model = xreg(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "assert type(xgb_model) == xgb.sklearn.XGBRegressor\n",
    "assert MSE(y_pred, y_test) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) CatFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите модель классификации катбуста на предложенных данных и верните обученную модель. \n",
    "\n",
    "Воспользуйтесь встроенной обработкой категориальных признаков. Не забудьте обработать Nan значения.\n",
    "\n",
    "Скрытых тестов нет, только один датасет `flyight_delays_train.csv` из гита 6 практики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catfeatures(df: pd.DataFrame):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/flight_delays_train.csv')\n",
    "df_train = df[:1000]\n",
    "model = catfeatures(df_train)\n",
    "\n",
    "\n",
    "assert type(model) == catboost.CatBoostClassifier\n",
    "\n",
    "df_test = pd.read_csv('data/flight_catfeature_test.csv')\n",
    "df_test = df_test.drop('Unnamed: 0', axis=1)\n",
    "X_test = df_test.drop('dep_delayed_15min',axis=1)\n",
    "y_test = df_test['dep_delayed_15min']\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "assert accuracy_score(y_test, y_pred) > 0.80 \n",
    "assert accuracy_score(y_test, y_pred) < 0.87 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вашем вниманию представляется прокаченный градиентный бустинг `LightGBM`. Разобраться в нем вам предлагается самостоятельно, например по [статье на хабре](https://habr.com/ru/company/skillfactory/blog/530594/). \n",
    "\n",
    "А в задачке, вам необходимо (опять...) найти наилучшие параметры для LGBMRegressor, обучить модель и вернуть ее. Данные из гита `Financial Distress.csv`.\n",
    "\n",
    "Сам гридсерч или нативное исследование необходимо делать вне функции обработки, чтобы не получить TL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbmreg(X_train, y_train):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Financial Distress.csv')\n",
    "\n",
    "X = df.drop('Financial Distress', axis=1)\n",
    "y = df['Financial Distress']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "\n",
    "lgbm_model = lgbmreg(X_train, y_train)\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "assert type(lgbm_model) == lightgbm.sklearn.LGBMRegressor\n",
    "\n",
    "assert MSE(y_pred, y_test) < 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Производные для регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окей, в лекции было очень много страшных формул, теперь можно осознать зачем это нужно.\n",
    "\n",
    "Пусть мы хотим бустить регрессию со стандартной функцией потерь $MSE$:\n",
    "\n",
    "$$\\mathcal{L}(a, x,y) = (a(x_i) - y_i)^2$$\n",
    "\n",
    "Необходимо найти через взятие производных:\n",
    "\n",
    "1. Константный вектор $[f_0]_{i=1}^{N}$\n",
    "$$f_0(x) = \\arg\\min_{ c\\in \\mathbb{R}} \\sum_{i=1}^n \\mathcal{L}(c, x_i, y_i)$$ \n",
    "\n",
    "2. Градиенты функции потерь\n",
    "$$g_{i}^{t} = -\\Big[\\frac{\\partial \\mathcal{L}(f_t, x_i, y_i)}{\\partial f_t(x_i)}\\Big]_{i=1}^N$$\n",
    "\n",
    "3. Коэффициенты при композиции \n",
    "$$\\alpha_{t + 1} = \\arg\\min_\\alpha \\sum_{i=1}^N \\mathcal{L}(f_{t} + \\alpha b_{t+1}, x_i, y_i)$$\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y = np.array([1, 2, 3])\n",
    "f = np.array([2, 2, 2])\n",
    "b = np.array([0, 2, 4])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "f_0 = 2.0\n",
    "g = [-2, 0,  2] \n",
    "alpha = 0.2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(y_i: np.array) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return f_0\n",
    "\n",
    "def grad(a: np.array, y: np.array) -> np.array:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return g\n",
    "\n",
    "def alpha(f :np.array, b: np.array, y: np.array) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3])\n",
    "f = np.array([2, 2, 2])\n",
    "b = np.array([0, 2, 4])\n",
    "\n",
    "f_0 = init(y)\n",
    "g = grad(f,y)\n",
    "al = alpha(f,b,y)\n",
    "\n",
    "assert np.abs(f_0 - 2.0)   < 1e-9\n",
    "assert_array_almost_equal(g, np.array([-2, 0, 2]))\n",
    "assert np.abs(al - (0.2)) < 1e-9\n",
    "######################################################\n",
    "y = np.arange(20)\n",
    "f = np.ones(20) * 10\n",
    "b = np.arange(20) - 1\n",
    "\n",
    "f_0 = init(y)\n",
    "g = grad(f,y)\n",
    "al = alpha(f,b,y)\n",
    "\n",
    "\n",
    "assert np.abs(f_0 - 9.5)   < 1e-2\n",
    "assert_array_almost_equal(g, np.arange(-20,20, 2))\n",
    "assert np.abs(al - (0.2748)) < 1e-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте градиентный бустинг на решающих деревьях для регрессии с логгированием.Верните модель, которая будет хранить в себе `n_estimatos` обученных деревьев и коэффициенты, чтобы с их помощью потом найти результат предсказания.\n",
    "\n",
    "Также необходимо реализовать логгирование в течение обучения.\n",
    "\n",
    "* `self.estimators` - лист c деревьями\n",
    "* `self.alpha` - лист с коэффициентами альфа\n",
    "* `self.f_list` - лист со значениями комбинаций алгоритма $f_T(x_i) = f_0(x_i) + \\sum_{t=1}^{T}\\alpha_tb_t(x_i)$\n",
    "* `self.g_list` - лист с векторами градиентов на каждой итерации $g_{i}^{t} = -\\Big[\\frac{\\partial \\mathcal{L}(f_t, x_i, y_i)}{\\partial f_t(x_i)}\\Big]_{i=1}^N$\n",
    "* `self.b_list` - лист со значениями базового обучаемого дерева на тренировочной выборке на каждой итерации \n",
    "\n",
    "Примечания:\n",
    "\n",
    "* Обрывать алгоритм не нужно, необходимо обучить все деревья.\n",
    "* Начальный константный вектор из $f_0$ логгировать не нужно, однако не забудьте его добавить в `predict` c нужным количеством объектов!\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "n_estimators = 2\n",
    "max_depth=3\n",
    "X_train = np.array([[0], [1], [2], [3], [4]])\n",
    "y_train = np.array([0, 2, 4, 2, 0])\n",
    "X_test  = np.array([[1.2], [2.3]])\n",
    "y_test  = np.array([2.2, 3.7])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_test_pred = [2, 4]\n",
    "\n",
    "model.f_list = [array([0.0, 2.0, 3.0, 3.0, 0.0]), \n",
    "                array([0.0, 2.0, 4.0, 2.0, 0.0])]\n",
    "\n",
    "model.g_list = [array([-3.2,  0.8, 4.8, 0.8, -3.2]), \n",
    "                array([ 0.0,  0.0, 2.0,-2.0,  0.0])]\n",
    "\n",
    "model.b_list = [array([-3.2, 0.8, 2.8,  2.8, -3.2]), \n",
    "                array([ 0.0, 0.0, 2.0, -2.0,  0.0])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MyGradBoost():\n",
    "    def __init__(self, n_estimators=10, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators_ = np.array([DTR(max_depth=self.max_depth) for _ in range(n_estimators)])\n",
    "        self.alpha = []\n",
    "        self.f_list = []\n",
    "        self.b_list = []\n",
    "        self.g_list = []\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train: np.array): \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X_test, y_test)-> np.array:\n",
    "        return mean_squared_error(self.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 2\n",
    "max_depth=3\n",
    "X_train = np.array([[0], [1], [2], [3], [4]])\n",
    "y_train = np.array([0, 2, 4, 2, 0])\n",
    "X_test  = np.array([[1.2], [2.3]])\n",
    "y_test  = np.array([2.2, 3.7])\n",
    "\n",
    "model = MyGradBoost(n_estimators=n_estimators, max_depth=max_depth).fit(X_train, y_train)\n",
    "assert model.score(X_test, y_test) < 0.2\n",
    "######################################################\n",
    "n_train, n_test, noise = 150, 1000, 0.1\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2)\\\n",
    "        + np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "\n",
    "model = MyGradBoost().fit(X_train, y_train)\n",
    "\n",
    "assert model.score(X_test, y_test) < 0.02\n",
    "\n",
    "\n",
    "model = MyGradBoost(n_estimators=100, \n",
    "                    max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "assert model.score(X_test, y_test) < 0.017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, X_test, y_test, title):\n",
    "    y_predict = model.predict(X_test)\n",
    "\n",
    "    plt.plot(X_test, f(X_test), \"b\")\n",
    "    plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "    plt.plot(X_test, y_predict, \"g\", lw=2)\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.title(\"{} Loss: {:2f}\".format(title, model.score(X_test, y_test)))\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "model = MyGradBoost(n_estimators=30, \n",
    "                    max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "ind =  [1,3,5,10,15,30]\n",
    "\n",
    "# GradientBoostingRegressor\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "n_est = [1,3,5,10,15,30]\n",
    "f = np.array(model.f_list)\n",
    "for i, n in enumerate(n_est):\n",
    "    colors = ['g', 'r', 'c', 'm', 'y', 'k']\n",
    "    plt.plot(X_train, f[n-1], color=colors[i], label=\"tree count={}\".format(n))\n",
    "\n",
    "plt.xlim([-5, 5])   \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) AdaBoost step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте одну итерацию алгоритма AdaBoost:\n",
    "\n",
    "1. Обучите дерево $b$ на $X_{train}$ и верните $y_{pred}$\n",
    "\n",
    "2. Найдите среднюю взвешанную ошибку\n",
    "$$error = Q(b_t, X, y) = \\frac{1}{N}\\sum_{i=1}^{N}w_i^{(t-1)}[y_i \\neq b_t(x)]$$\n",
    "\n",
    "3. Найдите коэффициент $\\alpha$ (для корректного выполнения добавим $eps$)\n",
    "$$\\alpha = \\frac{1}{2}\\ln\\Big(\\frac{1-error + eps}{error + eps}\\Big)$$\n",
    "\n",
    "4. Найдите новые веса:\n",
    "$$w_i^{new} = w_iexp\\Big(-\\alpha y_i b(x_i)\\Big)$$\n",
    "$$\\tilde{w}_i^{new} = \\frac{w_i^{new}}{\\sum_{i=1}^{N}w_i^{new}}$$\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\n",
    "y_train = np.array([-1, -1, -1, 1])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred =  [-1 -1 -1 -1] \n",
    "error = 0.056 \n",
    "alpha = 1.417 \n",
    "new_weights = [0.05882403 0.41176819 0.02941201 0.49999576]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boost_step(estimator, weights, X_train, y_train, eps = 1e-6):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return y_pred, error, alpha, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\n",
    "y_train = np.array([-1, -1, -1, 1])\n",
    "\n",
    "estimator = DTC(max_depth=1, random_state=4)\n",
    "sample_weights = [0.1, 0.7, 0.05, 0.05]\n",
    "y_pred, error, alpha, new_weights = boost_step(estimator, sample_weights, X_train, y_train)\n",
    "assert_array_almost_equal(y_pred, np.array([-1, -1, -1, -1])) \n",
    "assert np.abs(error - 0.056) < 1e-2 \n",
    "assert np.abs(alpha - 1.417) < 1e-2 \n",
    "assert_array_almost_equal(new_weights, np.array([0.05882403, 0.41176819, 0.02941201, 0.49999576]))\n",
    "######################################################\n",
    "X_train = np.array([[0, 0], [3, 3], [5, 5], [10, 10]])\n",
    "y_train = np.array([-1, -1, 1, 1])\n",
    "estimator = DTC(max_depth=1, random_state=6)\n",
    "sample_weights = [0.1, 0.7, 0.05, 0.05]\n",
    "\n",
    "y_pred, error, alpha, new_weights = boost_step(estimator, sample_weights, X_train, y_train)\n",
    "\n",
    "\n",
    "assert_array_almost_equal(y_pred, np.array([-1, -1, 1, 1])) \n",
    "assert np.abs(error - 0.0) < 1e-2 \n",
    "assert np.abs(alpha - 6.907) < 1e-2 \n",
    "assert_array_almost_equal(new_weights, np.array([0.11111111, 0.77777778, 0.05555556, 0.05555556]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "Реализуйте AdaBoost для бинарной классификации на деревьях высоты 1. Верните модель, которая будет хранить в себе `n_estimatos` обученных деревьев и коэффициенты, чтобы с их помощью потом найти результат предсказания.\n",
    "\n",
    "Также необходимо реализовать логгирование в течение обучения.\n",
    "\n",
    "* `self.sample_weights_list` - лист с весами объектов на каждой итерации\n",
    "* `self.y_pred_list` - лист с предсказанием каждого следующего дерева (не комбинации)\n",
    "* `self.error_list` - лист с ошибками\n",
    "\n",
    "Примечания:\n",
    "\n",
    "* Обрывать алгоритм не нужно, необходимо обучить все деревья.\n",
    "* Начальные веса логгировать не нужно\n",
    "* `predict_proba` реализовывать не нужно\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "n_estimators = 2\n",
    "X_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\n",
    "y_train = np.array([-1, -1, -1, 1])\n",
    "X_test  = np.array([[1, 0], [5, 5]])\n",
    "y_test  = np.array([-1, 1])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_test_pred = [0, 1]\n",
    "\n",
    "model.sample_weight = [array([0.167, 0.167, 0.167, 0.5]), \n",
    "                       array([ 0.1,  0.5,  0.1, 0.3])]\n",
    "model.y_pred = [array([-1, -1, -1, -1]),\n",
    "                array([-1,  1, -1,  1])]\n",
    "\n",
    "model.alpha = [0.25, 0.167]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "class MyAdaBoost():\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.estimators_ = np.array([DTC(max_depth=1) for _ in range(n_estimators)])\n",
    "        self.alpha = []\n",
    "        self.sample_weights_list = []\n",
    "        self.y_pred_list = []\n",
    "        self.error_list = []\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\n",
    "y_train = np.array([-1, -1, -1, 1])\n",
    "X_test  = np.array([[1, 0], [5, 5]])\n",
    "y_test  = np.array([-1, 1])\n",
    "\n",
    "model = MyAdaBoost(n_estimators = 2).fit(X_train, y_train)\n",
    "\n",
    "y_pred_my = model.predict(X_test)\n",
    "\n",
    "assert_array_almost_equal(y_pred_my, np.array([-1, 1]))\n",
    "\n",
    "######################################################\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([-1, -1, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6]])\n",
    "y_test  = np.array([-1, 1])\n",
    "\n",
    "model = MyAdaBoost(n_estimators = 2).fit(X_train, y_train)\n",
    "\n",
    "y_pred_my = model.predict(X_test)\n",
    "\n",
    "assert_array_almost_equal(y_pred_my, np.array([-1, 1]))\n",
    "######################################################\n",
    "num_samples = 1000\n",
    "theta = np.linspace(0, 2*np.pi, num_samples)\n",
    "r1 = 1\n",
    "r2 = 2\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "circle = np.hstack([np.cos(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8), \n",
    "                    np.sin(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8)])\n",
    "lil = r1 * circle\n",
    "big = r2 * circle\n",
    "X = np.vstack([lil, big])\n",
    "y = np.hstack([(-1) * np.ones(num_samples), np.ones(num_samples)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "y_pred_my = MyAdaBoost(n_estimators = 20).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert accuracy_score(y_pred_my, y_test) > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy Dataset\n",
    "x1 = np.array([.1,.2,.4,.8, .8, .05,.08,.12,.33,.55,.66,.77,.88,.2,.3,.4,.5,.6,.25,.3,.5,.7,.6])\n",
    "x2 = np.array([.2,.65,.7,.6, .3,.1,.4,.66,.77,.65,.68,.55,.44,.1,.3,.4,.3,.15,.15,.5,.55,.2,.4])\n",
    "y = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
    "X = np.vstack((x1,x2)).T\n",
    "\n",
    "boost = MyAdaBoost(n_estimators = 20).fit(X, y)\n",
    "sample_weight_list = np.array(boost.sample_weight_list)\n",
    "estimator_list = np.array(boost.estimators_)\n",
    "alpha = np.array(boost.alpha)\n",
    "\n",
    "fig = plt.figure(figsize = (14,14))\n",
    "for m in range(0,9):\n",
    "    fig.add_subplot(3,3,m+1)\n",
    "    s_weights = (sample_weight_list[m,:] / sample_weight_list[m,:].sum() ) * 40\n",
    "    plot_decision_boundary(estimator_list[m], X, y, N = 50, scatter_weights =s_weights )\n",
    "    plt.title('Estimator decision boundary, m = {}'.format(m))\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стэкинг** - 3-ий способ комбинирования алгоритмов, кроме бэггинга и бустинга. Он не часто используется, но его идея крайне полезная: `обучение на мета-признаках`.\n",
    "\n",
    "1. Разобъем нашу обучающую выборку на 2 части: базовую и дополнительную.\n",
    "2. Возьмем $N$ базовых алгоритмов и обучим их на **базовой части** разбив на $N$ фолдов. (Разбили на $N$ частей и обучаем алгоритм на всех частях кроме одной, как на кросс-валидации)\n",
    "3. Каждым из обученных базовых алгоритмов предскажем значение для **дополнительной** части выборки.\n",
    "4. Соберем **мета-выборку**, состоящую из предсказаний базовых алгоритмов на **доп выборе**. Пример: пусть для объекта $x_i$ базовые алгоритмы выдали $(y_i^1 = 1, y_i^2 = 0, y_i^3 = 1)$. Тогда признаками объекта в **мета-выборке** будет вектор $1, 0, 1$.\n",
    "5. Обучим **мета-алгоритм** на **мета-выборке**. И получим готовую модель.\n",
    "6. Чтобы получить результат на тестовой, теперь нужно сделать предсказания базовыми алгоритмами, собрать **мета-выборку** и сделать предсказания на **мета-алгоритме**.\n",
    "\n",
    "Реализуйте стекинг классификацию на **деревьях решений**. Валидация проводится на датасете `forest_train.csv` из папки в гите в 6 практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "class Stacking():\n",
    "    def __init__(self, n_estimators=5, max_depth=5):\n",
    "        self.max_depth_ = max_depth\n",
    "        self.n_estimators_ = n_estimators\n",
    "        self.estimators_ = [DTC(max_depth=self.max_depth_) for _ in range(self.n_estimators_)]\n",
    "        self.meta_estimator_ = DTC(max_depth=self.max_depth_)\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array): \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "df = pd.read_csv('data/forest_train.csv')\n",
    "\n",
    "X = df.drop(columns=['Cover_Type', 'Id']).reset_index(drop=True)\n",
    "y = df['Cover_Type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, train_size=0.3)\n",
    "\n",
    "model = Stacking(max_depth=10, n_estimators=3).fit(X_train, y_train)\n",
    "\n",
    "assert type(model.meta_estimator_) == sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred1 = model.estimators_[0].predict(X_test)\n",
    "y_pred2 = model.estimators_[1].predict(X_test)\n",
    "y_pred3 = model.estimators_[2].predict(X_test)\n",
    "\n",
    "assert accuracy_score(y_pred, y_test) > 0.67\n",
    "\n",
    "assert accuracy_score(y_pred1, y_test) < accuracy_score(y_pred, y_test)\n",
    "assert accuracy_score(y_pred2, y_test) < accuracy_score(y_pred, y_test)\n",
    "assert accuracy_score(y_pred3, y_test) < accuracy_score(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
